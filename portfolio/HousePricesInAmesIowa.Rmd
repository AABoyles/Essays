---
title: "House Prices in Ames, Iowa"
---

A common project in early statistics courses is to model house sales, based on [a famous dataset from Boston](https://archive.ics.uci.edu/ml/datasets/Housing). As an antidote to the commonness of this project, in 2011 Dean De Cock published a [rich dataset of house sales from Ames, Iowa](https://ww2.amstat.org/publications/jse/v19n3/decock.pdf). In late 2016, Kaggle picked up this dataset, launching the [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition.

So, let's see how well we can predict the sale prices.

```{r setup, include=FALSE}
library('knitr')
opts_chunk$set(message=FALSE, warning=FALSE)
```

# Data Plumbing

```{r readData, message=FALSE}
# Plyr is a defensive package load. Caret (which we use in the modeling)
# requires it sometimes, and we don't want to risk loading it after dplyr
# (which is loaded by tidyverse), so we'll just intentionally load it here.
library('plyr')
library('tidyverse')

set.seed(42)

if(!file.exists('../data-cache/HousePricesTrain.csv')){
  download.file(
    'https://storage.googleapis.com/kaggle-competitions-data/kaggle/5407/train.csv?GoogleAccessId=competitions-data@kaggle-161607.iam.gserviceaccount.com&Expires=1509220959&Signature=Ojje7az4R2NCwjxqB8RY8XXAVU9IQHi%2B9E6DqyyBwVatSYNLrE1OsAIwWhXgrb9klWQB9%2BtRJIoTeKd%2Fm5aq3mxMi%2B0WD9AbIsNCPDj%2BgYfyX5ESkWCg2a6bnyt328z7BH15vNr%2FUhH%2FF20esJxJnhBApbGO3sIrO10Pq%2FrorEuuM3WCALyOMW6QH5hz5KMMpyBisHbVht0w1qLdcFu%2BZFM3yJgHNHw%2Bvu5bXAfjgc9sncSRPqRn9CkShXaqmpCUjXxuQ4UpMpdKgbfc60ugpUM4CVUgcJyZpbZUg0WzOug6gkCDrADyQ2Yhgh5is2WGV6JWkW8LM2xu4C8nZPxaaw%3D%3D',
    '../data-cache/HousePricesTrain.csv'
  )
}

if(!file.exists('../data-cache/HousePricesTest.csv')){
  download.file(
    'https://storage.googleapis.com/kaggle-competitions-data/kaggle/5407/test.csv?GoogleAccessId=competitions-data@kaggle-161607.iam.gserviceaccount.com&Expires=1509221051&Signature=Yyvz9UucDqKB%2FVyMorG2j5xNlwkeYxorrppdCQHEhj7VotSs0WFA1xFv1Rp2m79R3S%2BRN5VapDuA623Fj0ozLmns1b0siriWQaDv1SFhN1edToQSl1bLrzPx0oSobGtwADQoQnNkjisrI7fdIh8W4BQO0ellX5K2nGAjhu7jjpcKP2Ac6Ex6coogwd7x7bhbV5K8bmAxvor8vTnx8ZDBMrMydpRd9HpQ0dNpVaGC4MEdMBTKkU6EQh3BA0jkK%2BcX8nQ5ywJ%2B8hdcM8pabjKi%2BG90AJcGdAJtxA5uqSjJ4TlWqZPttYgHHkQH1cXkttbHwA%2FwarE7ZEjJa%2FHdJXE0gg%3D%3D',
    '../data-cache/HousePricesTest.csv'
  )
}

training <- read_csv("../data-cache/HousePricesTrain.csv")
test <- read_csv("../data-cache/HousePricesTest.csv")
full <- bind_rows(training, test)
```

Before we delve into feature engineering, there are a lot of little nuisances in this dataset. Let's take care of those up here so they don't become a huge issue down there.

## Bad Names

Some of these columns have names that start with numerals. That makes R ...itchy. Let's just fix that right quick:

```{r renameVars}
library("magrittr")

full %<>%
  dplyr::rename(
    FirstFlrSF  = `1stFlrSF`,
    SecondFlrSF = `2ndFlrSF`,
    ThreeSsnPorch = `3SsnPorch`
  )
```

Note that this won't affect the models in any meaningful way.

## Missing Values

There are a ton of them.

```{r findMissing}
full %>%
  is.na() %>%
  colSums() %>%
  tbl_df %>%
  mutate(Variable = rownames(.)) ->
  temp
temp$Missing <- temp$value
temp %>%
  filter(Missing > 0) %>%
  dplyr::select(Variable, Missing) %>%
  arrange(desc(Missing)) %>%
  kable
```

They make the models fail. Let's do something about that.

First, let's excise variables which can't possibly contain enough information to inform a model in any useful way. There isn't a hard and fast threshold for this (it should vary widely, depending on the eccentricities of the specific dataset and models). In this case, I'm thinking that I'll cut out everything with more missing values than `LotFrontage` has.

```{r}
full %<>%
  select(-c(PoolQC, MiscFeature, Alley, Fence, FireplaceQu))
```

Typically, an `NA` in a dataset indicates that the data wasn't recorded for some reason. In this particular dataset, we can see that for many of these, the reason it that the values are literally "Not Applicable". A house has no garage, so any fields recording information about garages isn't applicable. Unfortunately, `NA` is a special value, as opposed to just another classification. So, let's comb through our non-numeric columns, and replace any instances of `NA` with a character string, "without". That way, we'll know without indicates that the house doesn't have the thing referenced by the column name, and we can still use it in our models.

```{r}
# This is my own personal concoction of stuff that makes writing R less miserable
# To replicate, install it like so:
# devtools::install_github('aaboyles/ShRoud')
library('ShRoud')

full %>%
  is.na() %>%
  colSums() %>%
  tbl_df() %>%
  mutate(Variable = rownames(.)) %>%
  filter(value > 0) ->
  VarsWithNAs

for (i in VarsWithNAs$Variable){
  if(is.character(full[,i][[1]])){
    full[is.na(full[,i]),i] <- "without"
  }
}
```

That takes care of the categorical missing data. However, we must also address missing numeric data as well. There are a ton of strategies we could take, from excluding training rows with missing values (probably a bad idea) to simple mean-substitution (a surprisingly effective, albeit unnuanced approach), all the way to statistical imputation. Let's do it that way:

```{r}
library('mice')

full %<>%
  select(-SalePrice) %>%
  mice(m=1, method='cart', printFlag=FALSE) %>%
  complete(action = 1, include = FALSE) %>%
  cbind(SalePrice = full$SalePrice)
```

Now, if we check for missing values, there aren't any (save for the obvious, missing `SalePrice` from the test data).

## Outcome Transformation

If we take a look at the distribution of our outcome metric...

```{r}
library('plotly')

training %>%
  plot_ly(x = ~SalePrice, type = "histogram")
```

You'll note that these values vary over at least two orders of magnitude (as [Alexandru Papieu pointed out](https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models)), so it may make more sense to predict the log-transformation of the data.

```{r}
training %>%
  mutate(SalePrice = log1p(SalePrice)) %>%
  plot_ly(x = ~SalePrice, type = "histogram")
```

Right, that's better.

```{r}
full %<>% mutate(SalePrice = log1p(SalePrice))
training <- full[1:nrow(training),]
test <- full[1+nrow(training):nrow(full),]
```

## Near-Zero Variance

The biggest problem I encountered in early modeling efforts was factors with values that occur infrequently in the data. Basically, what happens is we partition the data for cross-validation and there's a factor with one (or a few) especially rare value. All instances of that rare value land in the test data, so we have no way to assign a coefficient to it, and the model fails.

The simplest way to handle this is to drop any categorical variable with a value that is rarer than some tolerance threshold for model failure (basically, how patient you are). A better way to solve this problem is to translate ordinal variables onto a continuous scale. A computer can't figure out how far "good" is from "poor," but it can definitely figure out the difference between 4 and 1. That works adequately for ordinal variables, but its throwing away some discernable signal, and it works less well for nominal variables. For example, there is no inherent ordinality in countertop materials, but the market values granite more highly than formica. This is particularly instructive: instead of assuming that an ordinal variable follows its order, let's actually take the mean price for each category and see whether it follows the implied ordering.

So, let's take a category we'd otherwise throw away, and figure out how to numberify it.

```{r}
training$ExterCond %>%
  table() %>%
  tbl_df()
```

Perfect. With a 70-30 training-test partitioning, the probability that Po ("Poor") has no representatives in the training data is .3, which is totally unworkable. The generalized formula for that metric, by the way, is 

$$P(F) = (1-D_t)^{min(n)}$$ 

Where $F$ is the outcome in which the model fails because of rare values, $D_t$ is the proportion of the data used for training, and $n$ is taken from a table like the one above. To fix it, let's look at the mean house price for each member of the class:

```{r}
training %>%
  plot_ly(x = ~SalePrice, color = ~ExterCond, type = "histogram")
```

Here we can see that houses in Typical/Average shape on their exteriors actually fetch a slightly higher price, on average, than houses rated as being in "Good" shape. Interesting! We could do this in an automated way but replacing every categorical variable that has a sufficiently high probability of causing a modeling failure with the category's average SalePrice...

```{r, eval=FALSE}
# This code doesn't run-There's a bug in it and I don't actually care.

failureThreshold <- 1e-6

training <- full[1:nrow(training),]
test <- full[nrow(training)+1:nrow(full),]
for(column in colnames(training)){
  if(is.character(training[[column]])){
    # This is not a sane way to do this, but I don't know any better way.
    temp <- dplyr::group_by_(full, column) %>%
      dplyr::summarise(AveragePrice = mean(SalePrice, na.rm = TRUE))
    replacements <- as.list(temp$AveragePrice)
    names(replacements) <- temp[[column]]
    training %<>% mutate_(column = as.numeric(replacements[training[[column]]]))
    test     %<>% mutate_(column = as.numeric(replacements[test[[column]]]))
  }
}

full <- bind_rows(training, test)
```

...Or I could trust the good labor put in by [Stephanie Kirmer in this awesome Kernel](https://www.kaggle.com/skirmer/house-prices-advanced-regression-techniques/fun-with-real-estate-data/code).

```{r formatting_contour, collapse=TRUE}
full$flat[full$LandContour == "Lvl"] <- 1
full$flat[full$LandContour != "Lvl"] <- 0

full$pubutil[full$Utilities == "AllPub"] <- 1
full$pubutil[full$Utilities != "AllPub"] <- 0

full$gentle_slope[full$LandSlope == "Gtl"] <- 1
full$gentle_slope[full$LandSlope != "Gtl"] <- 0

full$culdesac_fr3[full$LandSlope %in% c("CulDSac", "FR3")] <- 1
full$culdesac_fr3[!full$LandSlope %in% c("CulDSac", "FR3")] <- 0

nbhdprice <- summarize(group_by(full, Neighborhood), mean(SalePrice, na.rm=T))

nbhdprice_lo  <- filter(nbhdprice, nbhdprice$`mean(SalePrice, na.rm = T)` < 140000)
nbhdprice_med <- filter(nbhdprice, nbhdprice$`mean(SalePrice, na.rm = T)` < 200000 &
                                   nbhdprice$`mean(SalePrice, na.rm = T)` >= 140000)
nbhdprice_hi  <- filter(nbhdprice, nbhdprice$`mean(SalePrice, na.rm = T)` >= 200000)

full$nbhd_price_level[full$Neighborhood %in% nbhdprice_lo$Neighborhood]  <- 1
full$nbhd_price_level[full$Neighborhood %in% nbhdprice_med$Neighborhood] <- 2
full$nbhd_price_level[full$Neighborhood %in% nbhdprice_hi$Neighborhood]  <- 3

full$pos_features_1[full$Condition1 %in% c("PosA", "PosN")] <- 1
full$pos_features_1[!full$Condition1 %in% c("PosA", "PosN")] <- 0

full$pos_features_2[full$Condition1 %in% c("PosA", "PosN")] <- 1
full$pos_features_2[!full$Condition1 %in% c("PosA", "PosN")] <- 0

full$twnhs_end_or_1fam[full$BldgType %in% c("1Fam", "TwnhsE")] <- 1
full$twnhs_end_or_1fam[!full$BldgType %in% c("1Fam", "TwnhsE")] <- 0

housestyle_price <- summarize(group_by(full, HouseStyle), mean(SalePrice, na.rm=T))

housestyle_lo  <- filter(housestyle_price, housestyle_price$`mean(SalePrice, na.rm = T)` < 140000)
housestyle_med <- filter(housestyle_price, housestyle_price$`mean(SalePrice, na.rm = T)` < 200000 &
                                           housestyle_price$`mean(SalePrice, na.rm = T)` >= 140000)
housestyle_hi  <- filter(housestyle_price, housestyle_price$`mean(SalePrice, na.rm = T)` >= 200000)

full$house_style_level[full$HouseStyle %in% housestyle_lo$HouseStyle]  <- 1
full$house_style_level[full$HouseStyle %in% housestyle_med$HouseStyle] <- 2
full$house_style_level[full$HouseStyle %in% housestyle_hi$HouseStyle]  <- 3

roofstyle_price <- summarize(group_by(full, RoofStyle), mean(SalePrice, na.rm=T))

full$roof_hip_shed[ full$RoofStyle %in% c("Hip", "Shed")] <- 1
full$roof_hip_shed[!full$RoofStyle %in% c("Hip", "Shed")] <- 0

roofmatl_price <- summarize(group_by(full, RoofMatl), mean(SalePrice, na.rm=T))

full$roof_matl_hi[full$RoofMatl %in% c("Membran", "WdShake", "WdShngl")] <- 1
full$roof_matl_hi[!full$RoofMatl %in% c("Membran", "WdShake", "WdShngl")] <- 0

price <- summarize(group_by(full, Exterior1st), mean(SalePrice, na.rm=T))

matl_lo_1  <- filter(price, price$`mean(SalePrice, na.rm = T)` < 140000)
matl_med_1 <- filter(price, price$`mean(SalePrice, na.rm = T)` < 200000 &
                            price$`mean(SalePrice, na.rm = T)` >= 140000)
matl_hi_1  <- filter(price, price$`mean(SalePrice, na.rm = T)` >= 200000)

full$exterior_1[full$Exterior1st %in% matl_lo_1$Exterior1st]  <- 1
full$exterior_1[full$Exterior1st %in% matl_med_1$Exterior1st] <- 2
full$exterior_1[full$Exterior1st %in% matl_hi_1$Exterior1st]  <- 3

price <- summarize(group_by(full, Exterior2nd), mean(SalePrice, na.rm=T))

matl_lo  <- filter(price, price$`mean(SalePrice, na.rm = T)` < 140000)
matl_med <- filter(price, price$`mean(SalePrice, na.rm = T)` < 200000 &
                          price$`mean(SalePrice, na.rm = T)` >= 140000)
matl_hi  <- filter(price, price$`mean(SalePrice, na.rm = T)` >= 200000)

full$exterior_2[full$Exterior2nd %in% matl_lo$Exterior2nd]  <- 1
full$exterior_2[full$Exterior2nd %in% matl_med$Exterior2nd] <- 2
full$exterior_2[full$Exterior2nd %in% matl_hi$Exterior2nd]  <- 3

full$exterior_mason_1[ full$MasVnrType %in% c("Stone", "BrkFace") |  is.na(full$MasVnrType)] <- 1
full$exterior_mason_1[!full$MasVnrType %in% c("Stone", "BrkFace") & !is.na(full$MasVnrType)] <- 0

full$exterior_cond[full$ExterQual == "Ex"] <- 4
full$exterior_cond[full$ExterQual == "Gd"] <- 3
full$exterior_cond[full$ExterQual == "TA"] <- 2
full$exterior_cond[full$ExterQual == "Fa"] <- 1

full$exterior_cond2[full$ExterCond == "Ex"] <- 5
full$exterior_cond2[full$ExterCond == "Gd"] <- 4
full$exterior_cond2[full$ExterCond == "TA"] <- 3
full$exterior_cond2[full$ExterCond == "Fa"] <- 2
full$exterior_cond2[full$ExterCond == "Po"] <- 1

full$found_concrete[full$Foundation == "PConc"] <- 1
full$found_concrete[full$Foundation != "PConc"] <- 0

full$bsmt_cond1[full$BsmtQual == "Ex"] <- 5
full$bsmt_cond1[full$BsmtQual == "Gd"] <- 4
full$bsmt_cond1[full$BsmtQual == "TA"] <- 3
full$bsmt_cond1[full$BsmtQual == "Fa"] <- 2
full$bsmt_cond1[is.na(full$BsmtQual)]  <- 1

full$bsmt_cond2[full$BsmtCond == "Gd"] <- 5
full$bsmt_cond2[full$BsmtCond == "TA"] <- 4
full$bsmt_cond2[full$BsmtCond == "Fa"] <- 3
full$bsmt_cond2[is.na(full$BsmtCond)]  <- 2
full$bsmt_cond2[full$BsmtCond == "Po"] <- 1

full$bsmt_exp[full$BsmtExposure == "Gd"] <- 5
full$bsmt_exp[full$BsmtExposure == "Av"] <- 4
full$bsmt_exp[full$BsmtExposure == "Mn"] <- 3
full$bsmt_exp[full$BsmtExposure == "No"] <- 2
full$bsmt_exp[is.na(full$BsmtExposure)]  <- 1

full$bsmt_fin1[full$BsmtFinType1 == "GLQ"] <- 5
full$bsmt_fin1[full$BsmtFinType1 == "Unf"] <- 4
full$bsmt_fin1[full$BsmtFinType1 == "ALQ"] <- 3
full$bsmt_fin1[full$BsmtFinType1 %in% c("BLQ", "Rec", "LwQ")] <- 2
full$bsmt_fin1[is.na(full$BsmtFinType1)] <- 1

full$bsmt_fin2[full$BsmtFinType2 == "ALQ"] <- 6
full$bsmt_fin2[full$BsmtFinType2 == "Unf"] <- 5
full$bsmt_fin2[full$BsmtFinType2 == "GLQ"] <- 4
full$bsmt_fin2[full$BsmtFinType2 %in% c("Rec", "LwQ")] <- 3
full$bsmt_fin2[full$BsmtFinType2 == "BLQ"] <- 2
full$bsmt_fin2[is.na(full$BsmtFinType2)] <- 1

full$gasheat[ full$Heating %in% c("GasA", "GasW")] <- 1
full$gasheat[!full$Heating %in% c("GasA", "GasW")] <- 0

full$heatqual[full$HeatingQC == "Ex"] <- 5
full$heatqual[full$HeatingQC == "Gd"] <- 4
full$heatqual[full$HeatingQC == "TA"] <- 3
full$heatqual[full$HeatingQC == "Fa"] <- 2
full$heatqual[full$HeatingQC == "Po"] <- 1

full$air[full$CentralAir == "Y"] <- 1
full$air[full$CentralAir == "N"] <- 0

full$standard_electric[ full$Electrical == "SBrkr" |  is.na(full$Electrical)] <- 1
full$standard_electric[!full$Electrical == "SBrkr" & !is.na(full$Electrical)] <- 0

full$kitchen[full$KitchenQual == "Ex"] <- 4
full$kitchen[full$KitchenQual == "Gd"] <- 3
full$kitchen[full$KitchenQual == "TA"] <- 2
full$kitchen[full$KitchenQual == "Fa"] <- 1

full$gar_attach[ full$GarageType %in% c("Attchd", "BuiltIn")] <- 1
full$gar_attach[!full$GarageType %in% c("Attchd", "BuiltIn")] <- 0

full$gar_finish[ full$GarageFinish %in% c("Fin", "RFn")] <- 1
full$gar_finish[!full$GarageFinish %in% c("Fin", "RFn")] <- 0

full$garqual[full$GarageQual == "Ex"] <- 5
full$garqual[full$GarageQual == "Gd"] <- 4
full$garqual[full$GarageQual == "TA"] <- 3
full$garqual[full$GarageQual == "Fa"] <- 2
full$garqual[full$GarageQual %in% c("Po", "without") | is.na(full$GarageQual)] <- 1

full$garqual2[full$GarageCond == "Ex"] <- 5
full$garqual2[full$GarageCond == "Gd"] <- 4
full$garqual2[full$GarageCond == "TA"] <- 3
full$garqual2[full$GarageCond == "Fa"] <- 2
full$garqual2[full$GarageCond == c("Po", "without") | is.na(full$GarageCond)] <- 1

full$paved_drive[ full$PavedDrive == "Y"] <- 1
full$paved_drive[!full$PavedDrive != "Y"] <- 0
full$paved_drive[is.na(full$paved_drive)] <- 0

full$housefunction[ full$Functional %in% c("Typ", "Mod")] <- 1
full$housefunction[!full$Functional %in% c("Typ", "Mod")] <- 0

full$sale_cat[full$SaleType %in% c("New", "Con")] <- 5
full$sale_cat[full$SaleType %in% c("CWD", "ConLI")] <- 4
full$sale_cat[full$SaleType %in% c("WD")] <- 3
full$sale_cat[full$SaleType %in% c("COD", "ConLw", "ConLD")] <- 2
full$sale_cat[full$SaleType %in% c("Oth")] <- 1

full$sale_cond[full$SaleCondition %in% c("Partial")] <- 4
full$sale_cond[full$SaleCondition %in% c("Normal", "Alloca")] <- 3
full$sale_cond[full$SaleCondition %in% c("Family","Abnorml")] <- 2
full$sale_cond[full$SaleCondition %in% c("AdjLand")] <- 1

full$zone[full$MSZoning %in% c("FV")] <- 4
full$zone[full$MSZoning %in% c("RL")] <- 3
full$zone[full$MSZoning %in% c("RH","RM")] <- 2
full$zone[full$MSZoning %in% c("C (all)")] <- 1

full %<>% select(-c(Street, LotShape, LandContour, Utilities, LotConfig,
  LandSlope, Neighborhood, Condition1, Condition2, BldgType, HouseStyle,
  RoofStyle, RoofMatl, Exterior1st, Exterior2nd, MasVnrType, ExterQual,
  ExterCond, Foundation, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1,
  BsmtFinType2, Heating, HeatingQC, CentralAir, Electrical, KitchenQual,
  GarageType, GarageFinish, GarageQual, GarageCond, PavedDrive, Functional,
  SaleType, SaleCondition, MSZoning))
```

Just as a Sanity check, did we introduce any new missing values?

```{r findMissing2}
full %>%
  is.na() %>%
  colSums() %>%
  tbl_df %>%
  mutate(Variable = rownames(.)) ->
  temp
temp$Missing <- temp$value
temp %>%
  filter(Missing > 0) %>%
  dplyr::select(Variable, Missing) %>%
  arrange(desc(Missing)) %>%
  kable
```

Crap. Let's just impute those again:

```{r}
full %<>%
  select(-SalePrice) %>%
  mice(m=1, method='cart', printFlag=FALSE) %>%
  complete(action = 1, include = FALSE) %>%
  cbind(SalePrice = full$SalePrice)
```

OK, that's done. Let's get to the fun part!

# Feature Engineering

## Year Built, Remodeled, Sold and Age

As our first foray into feature engineering, there's one funny little thing we don't know, but we can infer: the age of a house at the time it was sold.

```{r}
training %>%
  mutate(Age = YrSold - YearBuilt) %>%
  plot_ly(x = ~Age, y = ~SalePrice)
```

OK, so it looks like the age of a house is negatively correlated with price, which makes sense for several reasons. Obviously, older houses need more work. Anecdotally, it seems like older houses also tend to be smaller. Actually, we can check that:

```{r}
full %>%
  mutate(Age = YrSold - YearBuilt) %>%
  plot_ly(x = ~Age, y = ~GrLivArea)
```

OK, so that thing I said about older houses being smaller isn't actually true. Good to know. But what we actually need to figure out it whether or not the Age of the house contributes any signal that we don't get from other variables (especially the Year in which the house was built).

```{r}
library('intubate')
library('pander')

training %>%
  mutate(Age = YrSold - YearBuilt) %>%
  ntbt_lm(SalePrice ~ Age + YearBuilt) %>%
  summary() %>%
  pander()
```

OK, so Age doesn't really matter at all when in the same model as the absolute Year Built. Or rather, neither is significant, but one or the other (or both) explain about 34% of the variance in price. So, we should probably use one or the other, but not both. So, which one should we choose?

Let's suppose we update this data in 20 years, giving us decades of data rather than years. A house built in 1999 is always going to have been built in 1999, but in 2009 it will be 10 years old and in 2029 it will be 30 years old. As anyone who thinks about houses for two seconds will conclude, a newer house will likely require less maintenance than an older house, and thus be more desirable, and thus more expensive. In other words, Age should be robust across many years, while YearBuilt should degrade as houses get older. Accordingly, I believe Age is the superior metric.

What about other metrics we can derive from age? Maybe there's a quadratic relationship as well:

```{r}
training %>%
  mutate(
    Age = YrSold - YearBuilt,
    Age2 = Age^2
  ) %>%
  ntbt_lm(SalePrice ~ Age + Age2) %>%
  summary() %>%
  pander()
```

Cool. Can we go higher-order?

```{r}
training %>%
  mutate(
    Age = YrSold - YearBuilt,
    Age2 = Age^2,
    Age3 = Age^3
  ) %>%
  ntbt_lm(SalePrice ~ Age + Age2 + Age3) %>%
  summary() %>%
  pander()
```

Not really. Ok, so let's make sure at least Age and $Age^2$ are in the model.

```{r}
full %<>% mutate(
  Age = YrSold - YearBuilt,
  Age2 = Age^2
)
training <- full[1:1460,]
test <- full[1461:nrow(full),]
```

OK, so how about the time since it was Remodeled?

```{r}
training %>%
  mutate(AgeRemod = YrSold - YearRemodAdd) %>%
  plot_ly(x = ~AgeRemod, y = ~SalePrice)
```

Looks pretty striaghtforwardly linear. As remodels get older, prices decline.

```{r}
training %>%
  mutate(AgeRemod = YrSold - YearRemodAdd) %>%
  ntbt_lm(SalePrice ~ AgeRemod) %>%
  summary() %>%
  pander()
```

Cool. How about the year in which the house was sold?

```{r}
training %>%
  plot_ly(x = ~YrSold, y = ~SalePrice, type = "box")
```

Oh, this is fun: the data span the 2008 Housing Crash. Based on these box plots, it looks like the crash didn't drive a huge decline in prices for mid-level houses, but may have cut the long tail of more expensive sales. So, based on this, I wouldn't guess a model could do a ton with the sale year. Let's see:

```{r}
training %>%
  ntbt_lm(SalePrice ~ YrSold) %>%
  summary() %>%
  pander()
```

Yup. From a predictive perspective, this is actually possibly desirable. If there was a lot of variance in prices from year to year, those would add uncertainty to the model, tying it to other unknowns (like the health of the economy). Because this isn't (strongly) the case, we're better off not worrying about it. Moreover, it means our models should continue to make accurate forecasts for awhile, instead of degrading quickly because of whatever else changes over the years.

Another piece of information which may benefit us is the time of year. If you ever check on selling a property on Zillow, it will likely serve you a graph that looks something like this:

![](http://anthony.boyles.cc/imgs/zillow.png)

We also know the month in which the houses were sold. If Zillow's estimate is accurate, we may be able to discern some signal from the month of sale:

```{r}
training %>%
  plot_ly(x = ~MoSold, y = ~SalePrice, type = "box")
```

Eh, still no. If you focus on just the median lines, you can make out something like a relationship like the one Zillow claims, but I doubt it's worth chasing down.

## Size

It matters, people.

```{r}
training %>%
  plot_ly(x = ~GrLivArea, y = ~SalePrice)
```

Linear out of the box! Niiiiice.

```{r}
training %>%
  mutate(
    logGrLivArea = log(GrLivArea),
    sqrtGrLivArea = sqrt(GrLivArea),
    GrLivArea2 = GrLivArea^2
  ) %>%
  ntbt_lm(SalePrice ~ logGrLivArea + sqrtGrLivArea + GrLivArea + GrLivArea2) %>%
  summary() %>%
  pander()
```

Now, I tortured this until it started giving me any less than three stars, so there's a *lot* to be said for square footage. Into the model it goes!

```{r}
full %<>%
  mutate(
    logGrLivArea = log(GrLivArea),
    sqrtGrLivArea = sqrt(GrLivArea),
    GrLivArea2 = GrLivArea^2
  )
training <- full[1:1460,]
test <- full[1461:nrow(full),]
```

## Lot Features

```{r engineerFeatures}
full %<>%
  mutate(
    Baths = FullBath + HalfBath,
    BsmtBaths = BsmtFullBath + BsmtHalfBath,
    OverallQualSquare = OverallQual*OverallQual,
    OverallQualCube = OverallQual*OverallQual*OverallQual,
    OverallQualExp = expm1(OverallQual),
    TotalBsmtSFGrLivArea = TotalBsmtSF/GrLivArea,
    OverallCondSqrt = sqrt(OverallCond),
    OverallCondSquare = OverallCond*OverallCond,
    LotAreaSqrt = sqrt(LotArea),
    FirstFlrSFSqrt = sqrt(FirstFlrSF),
    TotRmsAbvGrdSqrt = sqrt(TotRmsAbvGrd)
  )
training <- full[1:1460,]
test <- full[1461:nrow(full),]
```

Note: I'll start cross-validating once I'm building models for prediction. These are just to give us a feel for whether or not a particular treatment (in this case, log-transformation) helps us.

```{r}
# Run this:
# "formula <- SalePrice ~ " %+% (names(full) %>% sort() %>% paste(collapse = " + "))
# ...to generate this:
formula <- SalePrice ~ Age + Age2 + air + Baths + BedroomAbvGr + bsmt_cond1 + bsmt_cond2 + bsmt_exp + bsmt_fin1 + bsmt_fin2 + BsmtBaths + BsmtFinSF1 + BsmtFinSF2 + BsmtFullBath + BsmtHalfBath + BsmtUnfSF + culdesac_fr3 + EnclosedPorch + exterior_1 + exterior_2 + exterior_cond + exterior_cond2 + exterior_mason_1 + Fireplaces + FirstFlrSF + FirstFlrSFSqrt + flat + found_concrete + FullBath + gar_attach + gar_finish + GarageArea + GarageCars + GarageYrBlt + garqual + garqual2 + gasheat + gentle_slope + GrLivArea + GrLivArea2 + HalfBath + heatqual + house_style_level + housefunction + Id + kitchen + KitchenAbvGr + logGrLivArea + LotArea + LotAreaSqrt + LotFrontage + LowQualFinSF + MasVnrArea + MiscVal + MoSold + MSSubClass + nbhd_price_level + OpenPorchSF + OverallCond + OverallCondSqrt + OverallCondSquare + OverallQual + OverallQualCube + OverallQualExp + OverallQualSquare + paved_drive + PoolArea + pos_features_1 + pos_features_2 + pubutil + roof_hip_shed + roof_matl_hi + sale_cat + sale_cond + SalePrice + ScreenPorch + SecondFlrSF + sqrtGrLivArea + standard_electric + ThreeSsnPorch + TotalBsmtSF + TotalBsmtSFGrLivArea + TotRmsAbvGrd + TotRmsAbvGrdSqrt + twnhs_end_or_1fam + WoodDeckSF + YearBuilt + YearRemodAdd + YrSold + zone

training <- full[1:nrow(training),]
test <- full[1+nrow(training):nrow(full),]

training %>%
  ntbt_lm(formula) %>%
  summary() %>%
  pander()
```
And, we're done! On to...

# Model the Data!

Now, to make a preliminary preparation, let's partition the data into training and test sets so we can do some of our own scoring without having to submit new entries to Kaggle all the time.

```{r dataPrep}
temp <- training %>% mutate(train = ifelse(runif(nrow(training))<.7, TRUE, FALSE))
subtrain <- temp %>% filter( train) %>% dplyr::select(-train)
subtest  <- temp %>% filter(!train) %>% dplyr::select(-train)
```

Also, I'm going to use Caret to fit the hyperparameters on models where that's useful, so I'm going to need a training controller for cross-validation.

```{r CVController}
library('caret')
library('parallel')
library('doMC')
fitControl <- trainControl(method = "repeatedcv", number = 5, repeats = 5)
registerDoMC(cores = detectCores() - 1)
```

## Linear Model

```{r lm1}
lm(formula, data=subtrain) %>%
  predict(subtest) %>%
  rmse(subtest$SalePrice)
```

Not bad for a first stab, so that's our baseline quality benchmark.

## ElasticNet

I thought about doing Ridge Regression or LASSO, but why do either when you can do both at once?

```{r ElasticNet}
subtrain %>%
  ntbt_train(formula, method = "glmnet", trControl = fitControl) %>%
  predict(subtest) %>%
  rmse(subtest$SalePrice)
```

## Cubist

This one will burn through a few cycles, caveat emptor.

```{r Cubist, eval=FALSE}
subtrain %>%
  ntbt_train(formula, method = "cubist", trControl = fitControl) %>%
  predict(subtest) %>%
  rmse(subtest$SalePrice)
```

## Random Forest

```{r RandomForest}
subtrain %>%
  ntbt_train(formula, method = "ranger", trControl = fitControl) %>%
  predict(subtest) %>%
  rmse(subtest$SalePrice)
```

## Gradient Boosting

```{r GradientBoosting}
subtrain %>%
  ntbt_train(formula, method = "gbm", trControl = fitControl, verbose = FALSE) %>%
  predict(subtest) %>%
  rmse(subtest$SalePrice)
```

## Make Some Predictions!

Let's rerun it on the entire Kaggle training set, predict on the test set, write and submit it.

```{r writeOut, eval=FALSE}
training <- full[1:1460,]
test <- full[1461:nrow(full),]

LM  <- predict(train(formula, data = training, method = "lm",     trControl = fitControl), test)
EN  <- predict(train(formula, data = training, method = "glmnet", trControl = fitControl), test)
C   <- predict(train(formula, data = training, method = "cubist", trControl = fitControl), test)
RF  <- predict(train(formula, data = training, method = "ranger", trControl = fitControl), test)
GBM <- predict(train(formula, data = training, method = "gbm",    trControl = fitControl), test)

test %>%
  cbind(LM, EN, C, RF, GBM) %>%
  mutate(SalePrice = expm1((LM + EN + C + RF + GBM) / 5)) %>%
  dplyr::select(Id, SalePrice) %>%
  write_csv("predictionMean.csv")
```

# One more thing...

> This is how you win ML competitions: you take other peoples’ work and ensemble them together.
-[Vitaly Kuznetsov](http://cims.nyu.edu/~vitaly/), NIPS2014

I collected the outputs of the four top-scoring public kernels. Let's see how they fare together.

```{r, eval=FALSE}
a <- read_csv('../data-cache/HousePrices/others/lasso.csv')
b <- read_csv('../data-cache/HousePrices/others/v6.csv')
c <- read_csv('../data-cache/HousePrices/others/v7.csv')
d <- read_csv('../data-cache/HousePrices/others/xgb.csv')
data.frame(
  Id = a$Id,
  SalePrice = (a$SalePrice + b$SalePrice + c$SalePrice + d$SalePrice)/4
) %>%
  write_csv('notmyensemble.csv')
```

Kaggle gave them up around .117, which is great, but not competitive.

# References

I owe a debt of gratitude to Stephanie Kirmer for [this Kernel](https://www.kaggle.com/skirmer/house-prices-advanced-regression-techniques/fun-with-real-estate-data), which was useful in guiding me through my own early data management and modeling.
