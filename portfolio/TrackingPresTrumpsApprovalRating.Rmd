---
title: "Tracking President Trump's Approval Rating"
---

```{r, message = F, warning = F, error = F}
library('knitr')
library('lubridate')
knitr::opts_chunk$set(message = F, warning = F, error = F)
source('../infobox.R')
infobox(list(
  `First Published` = today(),
  `Last Updated` = today()
))
```

# Introduction

As part of its [First 100 Days Challenge](https://www.gjopen.com/challenges/13-the-first-100-days), the [Monkey Cage blog](https://www.washingtonpost.com/news/monkey-cage/) asked the [Good Judgement Open](https://www.gjopen.com/), What will President Trump's approval rating be on April 28, 2017?

From the [question information](https://www.gjopen.com/questions/384-what-will-gallup-report-president-trump-s-net-approval-rating-to-be-on-28-april-2017):

> President-elect Trump's net approval rating has improved since his election in November ([Gallup](http://www.gallup.com/poll/201617/gallup-daily-trump-job-approval.aspx)). This question will be resolved by subtracting President Trump's disapproval rating from his approval rating using the "Gallup Daily" Trump job approval rating. Gallup is tracking President Trump's job approval ratings here: ([Gallup](http://www.gallup.com/poll/201617/gallup-daily-trump-job-approval.aspx)).

# Setting a Prior

For an uninformed prior, up to 100% of the population can approve of the President, and likewise 100% can disapprove. This means that valid net approval ratings span from -100 (everyone disapproves) to 100 (everyone approves). The span of -15 to 15 covers 30 of those 200 points, so if we assume that net approval ratings are pulled from a uniform distribution, we should expect to fall in that range with a frequency of around $30/200=.15$.

However, this is a crappy estimate for a lot of reasons. For one, in general, about half the country is going to disapprove of the President and about half the country will approve of the President. So the distribution of net approvals should have central tendency metrics (mean, mode) fairly close to 0. So, let's instead envision a bell curve with a mean of 0.

For another significant refinement to this conception of the distribution of approval ratings, day-to-day ratings seem to be correlated. For example, here's Barack Obama's approval ratings over his two terms:

![](http://anthony.boyles.cc/imgs/Screenshot_2017-03-16_13-35-53.png)

(Note that these are *approval* ratings, not *net approval* ratings. Gallup only launched the daily net approval rating for the Trump administration. However, since the net approval is the difference between Approval and a metric that is necessarily inversely proportional to it, looking at raw Approval ratings is still pretty instructive.)

As we can see, there are long term curves in the approval rating, but lots of day-to-day noise. For an even more dramatic perspective on this phenomenon, check out George W. Bush's approval ratings over his two terms:

![](http://anthony.boyles.cc/imgs/Screenshot_2017-03-16_13-36-24.png)

Here we can see President Bush enjoying a modest honeymoon period, followed by a huge jump on 9-11. From there, his approval began to degrade again until March 1, 2003, when he launched the US invasion of Iraq, causing another small spike in popularity. Everything else was a long, slow roll down the hill. We can point out these phenomenal changes because they were unusual. From day-to-day, the changes in approval don't swing widely across the range of possible approval estimates. If you want to predict the President's approval rating tomorrow, a decent strategy is to just guess what it is today.

So, what is President Trump's approval rating today?

# Getthing the Data

Let's parse the data directly from the Gallup Visualization's data feed:

```{r}
library('tidyverse')
library('xml2')
library('lubridate')

# Please excuse this XML hacking.
viz <- read_xml('http://www.gallup.com/viz/v1/xml/6b9df319-97d6-4664-80e1-ea7fe4672aca/POLLFLEXCHARTVIZ/TRUMPJOBAPPR201617.aspx') %>%
  xml_child(3) %>%
  xml_child() %>%
  xml_child(3) %>%
  xml_children() %>%
  as_list()

disapproval <- viz %>%
  lapply(function(el){el$p}) %>%
  flatten() %>%
  as.double()

approval <- viz %>%
  lapply(function(el){el[2]$p}) %>%
  flatten() %>%
  as.double()

n <- length(disapproval)

date <- ymd("17/01/21") + 1:n

ratings <- tibble(date, approval, disapproval) %>%
  mutate(net_approval = approval - disapproval)

ratings %>% tail() %>% kable()
```

Not great. If we guess that the President's approval rating will be what it is today, it's definitely inside the -15:15 band, but it's pretty close. How's he been doing?

```{r}
library('plotly')
plot_ly(ratings, x = ~date, y = ~net_approval, name = "Net Approval", type = 'scatter', mode = 'lines')
```

Not good at all. It seems President Trump has skipped the honeymoon and dived straight into the mid-40's doldrums which plague Presidents late in their terms.

To take a simple frequentist approach, let's just count the number of times his approval rating has fallen in each band.

```{r}
ratings %>%
  mutate(band = if_else(net_approval >= -15, "middle", "lower")) %>%
  select(-date, -approval, -disapproval, -net_approval) %>%
  table()
```

OK, since we only drop into the lower band twice, let's say that lower-band probability is something like `r round(2/n, 2)` and the middle-band probability is `r round(51/n, 2)`, and the high-band probability is very small.

I strongly dislike [probability estimates of 0 and 1](http://lesswrong.com/lw/mp/0_and_1_are_not_probabilities/), so instead of saying the high-band probability is zero, I'm going to invoke the [rule of three](https://en.wikipedia.org/wiki/Rule_of_three_(statistics)). The probability that the net approval is more than +15 must be less than `r round(3/n, 2)`. This is curious, as it implies the low-band and high-band probabilities are similar, despite the fact we've observed low-band outcomes and have not observed high-band outcomes. So, in the interest of preserving my tiny bias away from certainty, I'll call the high-band probability 1% and deduct it from the middle-band.

At this point, this is probably a pretty good basis for a GJO forecast. But can we do better?

# Real Forecasting

The "forecast" we computed above is a simple estimating of a probability distribution over outcomes for a single point. In Data Science Parlance, however, "forecasting" refers to something related but more general. Forecasting typically means to infer trends based solely on cyclicalities in the data. Data we care about tends to vary regularly across times of day, day of week, month of year, etc. If there's a long-term trend in these data, we may be able to catch wind of it by pitting our net approval data against an off-the-shelf forecasting tool. There's a whole class of interesting statistical tools to do this ([ARIMA](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average) is prehaps the most recognizable). Since it's shiny and new, let's try out Facebook's whiz-bang new time-series tool, [Prophet](https://facebookincubator.github.io/prophet/).

```{r}
library("prophet")

temp <- ratings %>%
  select(-approval, -disapproval) %>%
  rename(
    ds = date,
    y = net_approval
  )

m <- prophet(temp, mcmc.samples = 100)

future <- make_future_dataframe(m, periods = 100-n)

forecast <- predict(m, future)

plot_ly(forecast, x = ~ds, y = ~yhat_upper, name = "Upper bound", type = 'scatter', mode = 'lines') %>%
  add_lines(y = ~yhat, name = "Estimate") %>%
  add_lines(y = ~yhat_lower, name = "Lower Bound") %>%
  add_lines(data = temp, y = ~y, name = "Actual")
```

Yikes! Prophet strongly believes that President Trump is at the top of a downward trend, but one that will dramatically reverse itself sometime in late March. Given the limited amount of data available, it's actually encouraging to see the (80%) confidence intervals flare outwards like that. Even so, it's suspicious that Prophet thinks the President may even get a little positive net approval (something that happened only in the first days of the administration).

## Scoring the Prophet

In the Good Judgement Open, one doesn't really get credit for the binary accuracy of their forecasts. Rather, one is rewarded for minimizing error. So, let's measure Prophet's error:

```{r}
MAE <- (temp$y - forecast[1:n,]$yhat) %>%
  abs() %>%
  mean()
```

So, on average, the Prophet estimate has been off by about `r round(MAE, 2)` points. It's interesting to note that this is well within Gallup's own 3-point margin of error, so refining these predictions for higher precision doesn't make a lot of sense.

# Crunching out an Estimate

Let's start again with the base frequency-derived estimates: High ~1%, Middle 95%, Low ~4%. How should I update based on Prophet's estimate? Well, in all honesty, I don't know exactly. I don't know the distribution the Prophet assigns to its outcomes (I'm guessing it's normal). But I don't believe Prophet has much to find with the amount of data available yet. I could assume the distribution is normal and update my estimates using Bayes' formula, but the conclusion would be that the low and high-band probabilities are about equal, and tiny compared to the middle band. That's basically [what I believed at the start of this competition](http://www.gjopen.com/comments/comments/432692).

# Forecasting non-derived Signals

This is a slightly weird forecasting problem, as we're trying to forecast the outcome of a metric which is itself derived from two highly-correlated signals:

```{r}
library('pander')

model <- lm(approval ~ disapproval)

plot_ly(ratings, x = ~disapproval, y = ~approval, type = "scatter", name = "Observations") %>%
  add_trace(x = ~disapproval, y = ~fitted(model), mode = "lines", name = "LM")

model %>%
  summary() %>%
  pander()
```

So, perhaps we can do better by forecasting *each*, and then deriving the net approval from the forecast. There are good reasons not to do this! Think of a model as an engine. You feed it gasoline (empirical data) and it does work (good predictions), but with an undesirable side effect--it also emits toxic gas (error). By adding a second engine, we may produce twice as many predictions (which *doesn't actually help us*), but we'll also have two error generators (which actually hurts us).

That said, the two streams aren't perfectly correlated, and this is instructive. One may be acting in a way that is systematically (as opposed to randomly) distinct. Imagine two mobs of people. On the left, people who disapprove of the President. On the right, people who approve of him. Some people will move between the two mobs in response to things that happen. Some people will not. There's also an amorphous blob of people in the middle who don't know how they feel, or just don't feel strongly about the President. Here's a map of the political blogosphere that somewhat resembles what I'm envisioning:

![](http://adequatebird.com/wp-content/uploads/2010/05/Figure-1-Community-structure-of-political-blogs-expanded-set.jpg)

If people move between these camps independently, then maybe forecasting them separately makes sense. Let's just try it and see.

```{r}
dApproval <- data.frame(ds = date, y = approval)

m <- prophet(dApproval, mcmc.samples = 10000)

future <- make_future_dataframe(m, periods = 100-n)

forecast <- predict(m, future)

plot_ly(forecast, x = ~ds, y = ~yhat_upper, name = "Upper bound", type = 'scatter', mode = 'lines') %>%
  add_lines(y = ~yhat, name = "Estimate") %>%
  add_lines(y = ~yhat_lower, name = "Lower Bound") %>%
  add_lines(data = dApproval, y = ~y, name = "Actual")
```
