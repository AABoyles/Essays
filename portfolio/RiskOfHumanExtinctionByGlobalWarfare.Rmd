---
title: "Estimating the Risk of Human Extinction by Catastrophic Global Warfare"
---

# Introduction

```{r setup, message=FALSE}
library("HistData")
library("readr")
library("readxl")
library("magrittr")
library("dplyr")
library("tidyr")
library("fuzzyjoin")
library("intubate")
library("plotly")
library("ShRoud")
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

Most anthropogenic existential risks lack empirical evidence, and thus difficult to predict. (The AI Impacts project)

We have better data on the nature of natural existential risks. (Are near-Earth Objects Power-law distributed? Pandemics? Volcanic Eruptions?)

Warfare sits at the cross-section of these two conditions. It is both fundamentally anthropogenic, and a measurable phenomenon with a (sadly) rich historical record.

## Existential Risks


# The Probability of War

One of the most robust empirical properties of conflict (of which we are presently aware) is its power-law distribution (see for example [-@Bohorquez2009-jf]).

[Lewis Fry Richardson](http://en.wikipedia.org/wiki/Lewis_Fry_Richardson) figured out that incidents of violence follow a Power-Law Distribution [@Richardson1948-de]. Stated simply, the larger a war, the less common wars of at least that size will be.

Richardson also plotted this against estimates of the global murder rate, proposing that more-or-less all incidents of deadly violence were exemplars of a single, continuous phenomenon.

```{r Quarrels}
data(Quarrels)

Quarrels %>%
  group_by(monthsPairs) %>%
  dplyr::summarise(LogRangeOfMagnitude = round(mean(logDeaths))) %>%
  group_by(LogRangeOfMagnitude) %>%
  dplyr::summarise(LogNumberOfDeadlyQuarrels = log10(n())) %>%
  filter(LogRangeOfMagnitude >= 4) %>%
  rbind(data.frame(LogRangeOfMagnitude = c(3, 0, 0), LogNumberOfDeadlyQuarrels = c(2.1, 6.9, 7.1))) %>%
  plot_ly(x = ~LogRangeOfMagnitude, y = ~LogNumberOfDeadlyQuarrels, mode="markers")
```

(Note that the two points at the upper-left of the graph are estimates of the global murder rate, and the point with the LogRangeOfMagnitude of 3 is, as best I can tell, an estimate Richardson based on a linear extrapolation and not an actual measurement.)

The Correlates of War Project tracks conflict from 1800 to 2010.

```{r InterstateCOW}
## Download and Merge COW Datasets
if(!file.exists("../data-cache/interstate.csv")){
  read_csv("http://www.correlatesofwar.org/data-sets/COW-war/inter-state-war-data/at_download/file") %>%
    write_csv("../data-cache/interstate.csv")
}
interstatewars <- read_csv("../data-cache/interstate.csv") %>%
  mutate(StartYear2 = ifelse(StartYear2 < 1800, 3000, StartYear2)) %>%
  group_by(WarName) %>%
  dplyr::summarise(
    StartYear = min(c(StartYear1, StartYear2)),
    EndYear = max(c(EndYear1, EndYear2)),
    TotalCombatDeaths = sum(BatDeath)
  ) %>%
  mutate(Type = "Interstate")

wars <- interstatewars %>%
  dplyr::filter(TotalCombatDeaths > 1000) %>%
  dplyr::mutate(logDeaths = log10(TotalCombatDeaths)) %>%
  dplyr::arrange(desc(logDeaths)) %>%
  dplyr::mutate(logRank = max0(log10(1:n())))

fit <- lm(logRank ~ logDeaths, data = wars)

plot_ly(wars, x = ~logDeaths, y = ~logRank, type = "scatter", mode = "markers", color = ~Type,
 text = ~paste0(
   WarName, " (", StartYear, "-", EndYear, ")<br />",
   TotalCombatDeaths, " casualties")) %>%
 add_trace(x = ~logDeaths, y = fitted(fit), mode = "lines")
```

Thus far, it seems as though only *international* conflict has generated the conditions necessary for industrialized warfare of the scale that could constitute an existential risk (for example, a counterfactual thermonuclear exchange between the US and the Soviet Union). However, we can at least conceive of a set of circumstances in which a non-interstate political conflict initiates existentially risky conditions.

For example, the Soviet Union constructed an autonomous launching system for its nuclear missiles called the "Dead Hand". It is rumored to still be operational. A domestic terrorist could, hypothetically, execute a nuclear attack targeting some element of the Russian Government. This attack could create the conditions required to activate the Dead Hand (if it is still operational), leading to a dangerous thermonuclear assault (even if the other nuclear powers didn't respond in kind).

In other words, international conflict was the breeding ground for existentially risky weaponry, but it may be a subset of violent incidents, any one of which may scale upwards to an existential risk to humanity.

```{r IntrastateCOW}
if(!file.exists("../data-cache/intrastate.csv")){
  read_csv("http://www.correlatesofwar.org/data-sets/COW-war/intra-state-war-data-v4-1/at_download/file") %>%
    write_csv("../data-cache/intrastate.csv")
}
intrastatewars <- read_csv("../data-cache/intrastate.csv") %>%
  mutate(
    BatDeath = SideADeaths + SideBDeaths, 
    StartYear2 = ifelse(StartYear2 < 1800, 3000, StartYear2)
  ) %>%
  group_by(WarName) %>%
  dplyr::summarise(
    StartYear = min(c(StartYear1, StartYear2)),
    EndYear = max(c(EndYear1, EndYear2)),
    TotalCombatDeaths = sum(BatDeath)
  ) %>%
  filter(TotalCombatDeaths > 0) %>%
  mutate(Type = "Intrastate")
```

```{r ExtrastateCOW}
if(!file.exists("../data-cache/extrastate.csv")){
  read_csv("http://www.correlatesofwar.org/data-sets/COW-war/extra-state-war-data/at_download/file") %>%
    write_csv("../data-cache/extrastate.csv")
}
extrastatewars <- read_csv("../data-cache/extrastate.csv") %>%
  mutate(
    BatDeath = BatDeath + NonStateDeaths,
    StartYear2 = ifelse(StartYear2 < 1800, 3000, StartYear2)
  ) %>%
  group_by(WarName) %>%
  dplyr::summarise(
    StartYear = min(c(StartYear1, StartYear2)),
    EndYear = max(c(EndYear1, EndYear2)),
    TotalCombatDeaths = sum(BatDeath)
  ) %>%
  filter(TotalCombatDeaths > 0) %>%
  mutate(Type = "Extrastate")
```

```{r NonstateCOW}
if(!file.exists("../data-cache/nonstate.csv")){
  read_csv("http://www.correlatesofwar.org/data-sets/COW-war/non-state-war-data-1/at_download/file") %>%
    write_csv("../data-cache/nonstate.csv")
}
nonstatewars <- read_csv("../data-cache/nonstate.csv") %>%
  select(WarName, StartYear, EndYear, TotalCombatDeaths) %>%
  filter(TotalCombatDeaths > 0) %>%
  mutate(Type = "Nonstate")
```

```{r AllWarsCOW}
wars <- as.data.frame(interstatewars) %>%
  rbind(as.data.frame(intrastatewars)) %>%
  rbind(as.data.frame(extrastatewars)) %>%
  rbind(as.data.frame(nonstatewars)) %>%
  dplyr::filter(TotalCombatDeaths > 1000) %>%
  dplyr::mutate(logDeaths = log10(TotalCombatDeaths)) %>%
  dplyr::arrange(desc(logDeaths)) %>%
  dplyr::mutate(logRank = max0(log10(1:n())))

fitAll <- wars %>% ntbt_lm(logRank ~ logDeaths)

plot_ly(wars, x = ~logDeaths, y = ~logRank, type = "scatter", mode = "markers", color = ~Type,
 text = ~paste0(
   WarName, " (", StartYear, "-", EndYear, ")<br />",
   prettyNum(TotalCombatDeaths, big.mark = ","), " casualties"))
```

This presents a problem of counting. As Richardson observed, counting deaths by violence becomes increasingly difficult as you scale downwards [-@Richardson1948-de]. This is because of the comparative commonality of violence at small scales. It is tragically impractical to attempt to count all murders. Fortunately, the Correlates of War is not the only project to attempt to catalogue and measure political carnage. In response to this lack of data quality, Peter Brecke has committed heroic efforts to data collection, attempting to push the threshold of quality data on violent incidents down to skirmishes as small as 32 (that is, $10^{1.5}$) fatalities.

```{r ConflictCatalog}
# This probably won't work. Just open data-cache/ConflictCatalog18vars.xls in Excel and hit "save", and then try it.
if(!file.exists("../data-cache/ConflictCatalog18vars.xls")){
  download.file("http://www.cgeh.nl/sites/default/files/Conflict%20Catalog%2018%20vars.xls", destfile = "../data-cache/ConflictCatalog18vars.xls")
}
ConflictCatalog <- read_excel("../data-cache/ConflictCatalog18vars.xls")
```

To jump upwards towards the margins of measurable violence, [MURDER RATES]

http://www.cgeh.nl/sites/default/files/homicide.xlsx

Note the equation describing the regression model:

$$\log{P(S>s)}\approx2.017-.633 \log s$$

If we accept this model at face value, we must bring many assumptions along with it. Most prominently, we'll assume that wars are independent events, and that the technical ability to scale wars upward is static. Both of these are false, but provide a useful analytical ground upon which to build. In particular, we'll revisit the second assumption (regarding the technical capability to scale violence upward).

So, using this model, we can calculate the probability that a random war will end up being of a certain size (i.e. number of casualties). Let us consider the case of an existentially risky war--that is, one in which the entire population of humans was destroyed. At the time of writing (September 2016), the [US Census Bureau estimates](http://www.census.gov/popclock/?intcmp=home_pop) the Global Population to be around 7.338 Billion. The [UN estimates](https://esa.un.org/unpd/wpp/DataQuery/) that the global popultation in 2016 will eclipse 7.432 Billion. Let's split the difference and say that there are approximately 7.385 billion people.

Here we've accumulated a third important assumption: the global population is static. Again, this is false: it has consistently grown throughout recorded history, save for a moderate recession caused by the black plague. We will investigate this phenomenon in due course, but for now, let it suffice to assume that the global population is (and will continue to be) approximately stagnant at its current level. Setting the magnitude of of the relevant war ($s$), we get:

$$\log P(S>s)\approx2.017-.633 \log (7.385 * 10^9)$$

```{r, message=FALSE}
pOfEWar <- 10^(2.017-.633*log10(7.785e9))
```

Solving this equation for the given magnitude gives us the probability that any random war will reach the scale of an existential threat. This probability is $P(S>s)\approx`r pOfEWar`$

Note the word *random*. We can't calculate the probability that the big war is in our upcoming wars directly. However, we can calculate the probability that a given number of predicted wars does not contain WWIII (This is a variation on the [Birthday Problem](http://en.wikipedia.org/wiki/Birthday_problem).) The probability of the first war being existentially risky is approximately $`r pOfEWar`$. Because these wars are independent (an assumption which is inextricable from the model), the probability of the second war being existentially risky is also $`r pOfEWar`$, and so on for all subsequent conflicts. The probability that neither the first war *nor* the second war is existentially risky is $(1-`r pOfEWar`)^2 \approx`r (1-pOfEWar)^2`$. Generalizing this formulation, we get the probability that an existentially risky war does not occur within a given number ($n$) of wars:

$$P(S>s|n)\approx(1-10^{2.017-.633\log s})^n$$

From here, it's just a small logical step to the probability that a war of given magnitude occurs within $n$ wars.

$$P(S<=s|n)\approx1-(1-10^{2.017-.633\log s})^n$$

[Note the change in syntax on the left side of the formula, indicating that every war $S_i$ in a set of $n$ wars is smaller than a hypothetical war $s$] So, how far do we have to look to see the War to end all wars (by virtue of ending humanity)? More or less fortunately, we cannot estimate this directly: we can only estimate probabilities over numbers of wars. Using our previous estimates for the magnitude of an existentially risky war:

$$P(S<=s|n) = .5 \approx 1- (1 - `r pOfEWar`)^n$$

We can plot this function:

```{r}
#TODO: Fix the Axis Titles
plot_ly(x = 1:100*1000, y = 1-(1-pOfEWar)^(1:100*1000))
```

So, for example, let's say we're curious about the time frame for which an existentially risky war is 50% likely. Solving for $n$ gives us $n \approx 12160.13$ (which we can confirm visually using the above plot). So an existentially risky war should occur (with probability .5) within 12161 wars or so.

## Black Swan Wars

The fact that no globally catastrophic war has occurred should not be misconstrued as strong evidence that such a war is unlikely.

## "Dragon King" Wars

[Dragon King Events]

In this case, we should not view existentially risky warfare as a single, huge, unlikely war, but as the sum of massive wars which are much likelier than simple inference from power laws suggests.

# Population Dynamics

In general, predicting the size of a population is a fraught modeling exercise. On the one hand, we have extremely good information about the means by which populations grow (i.e. births) and shrink (deaths), and demographers have been conducting sophicticated research for a long time. On the other hand, measuring these is a difficult task. Prior to 1950, we have more-or-less just good guesses, born out of heroic research efforts.

We should treat any conclusions which rely upon precise estimates of the global population as suspect. For present purposes, simply being within about 10% of the "correct" value should suffice. and our confidence in the estimates drops significantly as we look back in time further than around 1950. Never-the-less, many have attempted to construct meaningfully useful estimates, and these tend to be largely in agreement:

```{r populationData, message=FALSE}
if(!file.exists("../data/PopEstimates.csv")){
  #TODO: Throw an Error
}

pop <- read_csv("../data/PopEstimates.csv") %>%
  filter(Year > 1800, Year < 2016)

pop %>%
  gather(Estimator, Estimate, `United States Census Bureau (2015)`:`Kapitza (1996)`) %>%
  plot_ly(x=~Year, y=~Estimate, color=~Estimator) %>%
  layout(legend = list(x = 0, y = 1))
```

One which I find particularly worth highlighting is [Sergey Kapitza's phenomonological model of global population growth](https://web.archive.org/web/20090511041230/http://srs.dl.ac.uk/SPEAKERS/KAPITZA/Uspekhi_96.html), which can be roughly stated as:

$$N\approx4.43*10^9*\arctan{\frac{42}{2007-Y}}$$

where $N$ is the estimate of the Global Population, and $Y$ is the year.

Rather than rely exclusively upon any of these estimates, I've opted to merge and average them.

The global population model is a simple gain-loss model.  Gains are defined by births, and losses by deaths.  It's predicated on the fact that are only these two ways to change the global population (i.e. humanity's total population count is unaffected by migration, [divine ascension](http://en.wikipedia.org/wiki/Entering_heaven_alive), etc...).

$$B_n=P_n(\Delta B_n)$$

Where $B_n$ is the number of births at time $n$, $P_n$ is the Population at time $n$, and $\Delta B_n$ is the birthrate at time $n$. 

$$D_n=P_n(\Delta D_n)$$

$$P_{n+1}=P_n+B_n-D_n$$

Thus,

$$P_{n+1}=P_n(1+\Delta B_n-\Delta D_n)$$

Now, in order to define these numerical values, we must consult some statistics.  For this, there is the [World Development Indicators](http://data.worldbank.org/indicator?display=graph).  I parsed the very same into separate datasets not so long ago, but this time it'll be easier to just use the graphs.  For that, Google has set up a really nifty [Public Data Explorer](http://www.google.com/publicdata/directory).  To start with, let's look at [Birthrate](http://www.google.com/publicdata/explore?ds=d5bncppjof8f9_&ctype=l&strail=false&nselm=h&met_y=sp_dyn_tfrt_in&scale_y=lin&ind_y=false&rdim=country&ifdim=country&tstart=-291844800000&tend=1285992000000&uniSize=0.035&iconSize=0.5&icfg).  The graph looks pretty nonlinear, the take home point is that it's trending downwards.  As such, we can represent it linearly with the knowledge that this may be something to revisit later on the the model construction.  Over the 49 year period from 1960-2009, the value drops from 4.91 to 2.52.  That's a change of  2.39, or a mean change of .049 annually.  But what does this number actually mean?

> The average number of births per woman.

This creates two new problems.  The first is pretty clear: We will know how many people there are at any given time, but not how many women.  We'll have to include another converter to capture this fact.  The second is much more subtle: this is evaluated over steps of time (in this case, years).  But the rate is measured over a woman's lifespan.  So we also must include a variable which represents the mean female lifespan.  All told, here's our model:

$$L_n=54.6+.345n$$

$$\Delta B_n=\frac{4.91-.049n}{L_n}$$

Where the numerical values are derived from time series regressions on the data for [female life expectancy](http://data.worldbank.org/indicator/SP.DYN.LE00.FE.IN/countries?display=graph) and [fertility rate](http://data.worldbank.org/indicator/SP.DYN.TFRT.IN/countries?display=graph).  As for the [percent of the population which is female](http://data.worldbank.org/indicator/SP.POP.TOTL.FE.ZS/countries/1W?display=graph), it looks to be roughly stagnant around 49.7, so a fixed value should be good enough. Now all that remains is to define the Deathrate and test the model.  Deathrate presents a new problem--a [truly nonlinear relationship](http://data.worldbank.org/indicator/SP.DYN.CDRT.IN/countries/1W?display=graph).  As such, we'll divide it by 1000 to give us a per capita probability of death per year, and fit the prediction curve using exponential regression, giving us this equation:

$$\Delta D_n=\frac{.018907}{n^{0.21}}$$

With all of our values set and relationships defined, its high time we tested this puppy out!  We'll set the initial population equal to the population in 1960, [approximately 3 billion](http://www.google.com/publicdata/explore?ds=d5bncppjof8f9_&ctype=l&strail=false&nselm=h&met_y=sp_pop_totl&scale_y=lin&ind_y=false&rdim=country&ifdim=country&tstart=-291758400000&tend=1286078400000&uniSize=0.035&iconSize=0.5&icfg).  We should see population growth up to today's estimate of 6.775 billion.  Let's check it out, shall we?

Two dangling observations: First, we started out modeling a phenomena which is strikingly linear.  Second, our prediction is slightly nonlinear.

1. The life expectancy is latent--it describes the life expectancy of people _born_ in that year, not the life expectancy of people _living_ in that year.
2. We didn't describe the female percentage of the population with a function, which makes it entirely inflexible.
3. The pseudo-linear models we use to generate many of the coefficients are imperfect (though admittedly very good in this case).
4. The data are imperfect: the World Development Indicators are calculated using a completely different methodology, which one hopes would be fairly accurate.  Even so, it would be logistically impossible to count every single person on the planet at a fixed point in time--people are born and die every minute.  The WDIs are correct to the ballpark (as are the models we devise from them), but only so.

Even so, I claim we've accomplished something moderately cool.  We've set up a complex equation set which predicts the global population, controlling for life expectancy, fertility, proportion of the population which is female, and per capita deathrate.  Our model could survive an exogenous shock to any of these, while the linear model is left scratching its head.  System Dynamic modeling is cool because it is necessarily representative of the causal linkages between variables, thereby creating much more compelling forecasts.  This model hinges upon the irrefutable causality between births, deaths, and population size.

As a follow-up to  on the measurement of the global population using system-dynamic modeling and regression techniques on data from the World Bank, I decided to point my model in the opposing direction.  I mean we can use exactly the same model to forecast the population into the future. To start, we must find some data to compare against.  The U.N. publishes [a biannual prediction](http://esa.un.org/unpd/wpp/index.htm) of the global population trajectory.  The employ three creatively-named models: "high," "medium," and "low."  High is a worst-case scenario.  It's basically a prediction that [Malthus](http://en.wikipedia.org/wiki/Thomas_Robert_Malthus#An_Essay_on_the_Principle_of_Population) was right, and populations grow exponentially.  Medium is a little sunnier--it's really the world we want to live in.  [The idea](http://www.ted.com/talks/hans_rosling_on_global_population_growth.html) is that around 2050 we round out to about 9 Billion and we stay there, more or less.  High would be inconvenient--lots of starvation, bad sanitation, generally bad conditions with little to do to improve them, but Low is the real downer.  What if something went catastrophically wrong?  War, disease, disaster--take your pick.  Low is the image of the world picking up the pieces. Against these models, I will compare two of my own: the system dynamic model I built, and the linear model that outperformed it.  By the way, that linear model is strikingly simple:

$$P_t=76254816t+6895889018$$

In which t is time (i.e. years since 2009) and the intercept is the approximate 2009 population.  So, let's take a look at the next 90 years: Interesting.  We have a uniform, fanning-out effect.  That means we all disagree pretty enthusiastically.  Only the low model begins to fall, while the medium converges on 9 Billion, and the top three forecasts (mine, the linear model, and the UN's high, respectively in increasing order) continue to increase at varying speeds.  OK, what if we look a little farther forward?  Back in 2004 the UN published [a projection to 2300](http://www.un.org/esa/population/publications/longrange2/WorldPop2300final.pdf).  Let's see what the world looks like then: Now this is interesting: Low and medium have both held their positions for a couple of centuries, and it looks like mine is getting to a point of convergence, but linear keeps growing (as linear models are wont to do) and high is skyrocketing.  We must have cured death or something at that point. There are important sources of distinction between my model and the U.N.'s.  The UN possesses an immediate advantage in that its models may start projecting based on data which is empirically accurate for the current year.  Because my model was calibrated on the data from the prior 50 years, I must start with the year 1960 and project to 2010 and beyond.  This means the number I have for this year's population is probably a little inflated, so the model on the whole is relatively high.  The other is that my model continues to project expectations into the realm of unlikelihood--by 2300, my model suggests the mean female lifespan should be over 100, and still rising.  Even so, it may accidentally prove to be the most accurate of them all, if for entirely spurious reasons.  As they say, "All models are wrong." Last night's post brought me a to final question on [the population model](http://nortalktoowise.com/2011/10/measuring-global-population-using-system-dynamics/) [with which I've been tinkering](http://nortalktoowise.com/2011/10/forecasting-the-global-population/).  Will my model converge?  There are two ways to answer this question.  The first is kinda sorta calculus.  To start, we must collapse all those equations from the model into a single function.  We can do this because each equation in the model is contingent only upon other values in the system (none of which are feedback process except the canonical Births and Deaths), time (represented as the variable _n_), or the dependent variable, _P_.  So the final function should show the relationship between time and Global Population. First, the global population is equal to last year's population plus the births minus the deaths.  The quintessential gain-loss model.

$$P_n=P_{n-1}(1+B_n-D_n)$$

Let's start by unwrapping _Births_ (actually _births per capita_), which should be the more complicated of the two:

$$B_n=F \Delta B_n$$

Where $F$ is the proportion of the human population which is female.

$$\Delta B_n=\frac{4.91-.049n}{L_n}$$

Recalling that:

$$L_n=54.6+.345n$$

We can combine these equations, yielding:

$$B_n=\frac{.497(4.91-.049n)}{54.6+.345n}$$

Now, for _death_, if we remove the _P_ variable the way we did for _births_ (since it was pre-factored out in the equation 1), _deaths_ (again, deaths per capita) is simply equal to the _deathrate_.  So,

$$D_n=.018907n^{-0.21}$$

Thus, if we put this all together:

$$P_n=P_{n-1}*(1+\frac{.497*(4.91-.049n)}{54.6+.345n}-.018907n^{-0.21})$$

Since we can't easily calculate P into infinity this way (without some heavy-duty computation), let's just think about the second half of the expression, and measure what happens as _n_ approaches infinity.  If the output (which is basically a global population multiplier) hits 1, then an equilibrium state exists.  If not, then the global population should continue to fluctuate forever.  Let's see:

$$f(n)=1+\frac{.497*(4.91-.049n)}{54.6+.345n}-.018907n^{-0.21}$$

$$\lim_{n \to \infty}f(n)=1+0-0$$

Interesting.  Both Births and Deaths converge on 0, so the equilibrium state exists.  How big is it?  How long will it take? Equilibrium is the point at which the global population no longer changes from year to year, or if it does, it begins to do so cyclically.  There aren't any cyclical parameters in the model, and there aren't enough independent variables for cyclicality to emerge organically, so my suspicion is that we should see a single point of convergence.  In other words, the global population will reach some number and stick to it.  Algebraically, we can represent this as:

$$P_{n}=P_{n-1}$$

or, since we're solving for _n*_, the equilibrium condition, we know P at _n_ is equal to the P at _n_-1\.  Thus we can divide both sides of the equation by P and voila!

$$1=1+\frac{.497*(4.91-.049n)}{54.6+.345n}-.018907n^{-0.21}$$

$$0=\frac{.497*(4.91-.049n)}{54.6+.345n}-.018907n^{-0.21}$$

$$.018907n^{-0.21}=\frac{.497*(4.91-.049n)}{54.6+.345n}$$

We have now reached a substantively interesting and intuitively obvious point: equilibrium is the state of the population in which the number of annual births _equals_ the number of deaths.  Moving on:

$$.018907n^{-0.21}(54.6+.345n)=.497*(4.91-.049n)$$

$$1.0323222n^{-0.21}+.006523n^{.79}=2.44027-0.024353n$$

$$1.0323222n^{-0.21}+.006523n^{.79}+0.024353n-2.44027=0$$

Recall that this was analysis predicated on the assumption that the global population will endure many wars without changing significantly. This is obviously false: any war will reduce the population, but by a tiny amount relative to the speed of population growth. The global population has grown significantly in the last 200 years. As the global population grows, the probability of an existentially risky war drops. This is partly because the trend of the probability of a war is inversely correlated with the number of casualties that war will cause.

## Growth

## Stagnation

[Malthus]

## Decline

As birth control has become available, fertility rates have dropped, well below the replacement rate in some advanced economies. There is a possibility that humanity will experience a natural population decline in the near-distant future.

# Trends in Violence

Thus far, our models have been largely devoid of empirical content regarding the character of wars and the state of the world which causes them.

[The Better Angels of Our Nature]

Violence itself is trending downward. It's not at all clear why.

A decline in violence depresses the probability of wars.

## Mechanisms of the Onset of Warfare

This is an extremely fraught and difficult effort. The specific, substantive sources of potentially massive conflict are likely to change on the order of decades, if not years. Even so, 

[Backing into World War III](http://foreignpolicy.com/2017/02/06/backing-into-world-war-iii-russia-china-trump-obama/)

### NATO-Russia

[How World War III Became Possible](http://www.vox.com/2015/6/29/8845913/russia-war)

[How Likely is war between the United States and Russia in the next 10 years?](https://trip.wm.edu/charts/#/bargraph/3/26)

[How Likely Is (Nuclear) War Between the United States and Russia?](https://dartthrowingchimp.wordpress.com/2015/07/03/how-likely-is-nuclear-war-between-the-united-states-and-russia/)

[How Likely is nuclear war with Russia?](http://www.vox.com/2015/7/6/8900237/russia-war-odds)

### US-China

[How Likely is war between the United States and China in the next 10 years?](https://trip.wm.edu/charts/#/bargraph/3/25)

# The Effects of Technological Progress

Humanity has only recently developed the technology to cause disasters on the scale of the entire population. Prior to the dawn of flight, ground and naval warfare only meaningfully threatened millions of people at once. Prior to the dawn of the nuclear era, industrialized warfare only threatened hundreds of millions. This hints at an possibly informative trend: the growth in human ability to scale destruction. Specifically, is humanity's ability to scale warfare growing faster than its population growth? If so, then the probability of an existentially risky war may be growing, even as the probability of any war continues to shrink. If not, then an existentially risky war may not actually be meaningfully possible until population trends change.

Lars Erik Cederman [replicated this result](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.143.8098&rep=rep1&type=pdf) in a paper in which he took the casualty estimates from the [Correlates of War](http://www.correlatesofwar.org/).

```{r}
## Merge in Annual Global Population Estimates
## Show largest body count over time
## Is our ability to scale warfare growing faster than our population growth?
```

## Bigger Nukes

While it's widely believed that there are a sufficient number of nuclear weapons to extinguish humanity, we have yet to see an explosive weapon of the requisite magnitude to achieve human extinction by itself. The details of such a device are purely conjectural. A 50 Gigaton weapon should be sufficient to engulf the area of the inhabited Earth.

Consider a single 50GT doomsday weapon. It may be able to engulf the inhabited area of the Earth on a flat plane, but planetary geometry is likely to prevent such a weapon from achieving its objective. As the fatal effects of the weapon expand outwards spherically, the surface of the Earth will block those effects from reaching the opposing hemisphere. Rather than attempting to build a single doomsday weapon, a more effective approach would be to build two, and place them at any anti-podal locations on the globe. Weapons of approximately 15 GT should be able to accomplish this.

At present, the largest weapon ever tested was the Tsar Bomba, a 50 MT thermonuclear weapon tested by the Soviet Union. A single doomsday bomb (or two twin doomsday bombs) would require three orders of magnitude more yield. It is conceivable that such a weapon could be constructed. Nuclear yields scaled over three orders of magnitude in the 16 years between the Trinity test (1945, 20KT) and the Tsar Bomba (1961). The fact that nuclear yields ceased to grow at this point is noteworthy.

Diminishing returns on investment. A shift to more precise targeting?

## Biowarfare

One obvious alternative to apocalypse by nuclear weapons is apocalypse by biological weapons. A combatant nation may engineer some virus or organism to induce a fatal illness with a high transmission rate.

The biological agent evolves immunity to the antidote.

...Or there wasn't an antidote.

## Unfriendly Artificial Superintelligence

Cybersecurity, blah blah blah.

Military develops offensive cyber agent, accidentally detonates Intelligence Explosion. Agent persues Omohundro drives, destroys humanity in the process.

## Unknown Unknowns

Former Secretary of Defense Donald Rumsfeld famously said:

> Reports that say that something hasn't happened are always interesting to me, because as we know, there are known knowns; there are things we know we know. We also know there are known unknowns; that is to say we know there are some things we do not know. But there are also unknown unknowns – the ones we don't know we don't know. And if one looks throughout the history of our country and other free countries, it is the latter category that tend to be the difficult ones.

New, heretofore unknown means of warfare could constitute existential risks. Because these possibilities are unknown, we must venture deep into speculative territory for possible examples. One such is that an Ice-nine-like arrangement of matter could be discovered, and then ham-handedly employed in battle. Alternately, it could be attached to a tripwire mechanism, not unlike the Soviet Union's Dead Hand.

This substance could be new discoveries in high-energy physics could result in processes for generating stable strangelets on-demand. These processes could be weaponized.

# The Anthropic Logic of Peace

The Cold War marked an important threshold in the history of the warfare: namely, the first time it which it became conceivable that humanity could launch a war it might not survive.

It is also conceivable that the effects of direct nuclear strikes in concert with nuclear fallout and the subsequent nuclear winter might extinguish humanity with very high probability, and reduce the number of observer-moments on Earth with practical certainty. By this reasoning, it should not seem surprising that the world emerged from the cold war unscathed, because vastly more observer-moments occur in situations in which the Cold War does not lead to nuclear exchanges.

What this should communicate is that the probability of a catastrophic nuclear war may have been much higher, and we only survived as a fluke. The history of nuclear accidents suggests that this is possible, and perhaps even likely.

[Cuban Missle Crisis]

[Stanislav Petrov]

[Vasily Arkhipov]

My personal assessment of the probabilities of these events resulting in a thermonuclear war far exceeds those suggested by the statistical models outlined above. How can these contradictory probabilities be reconciled?

The answer is a question of anthropic logic. In his doctoral dissertation, Nick Bostrom defines anthropic reasoning as a misnomer for 

## The Sleeping Beauty Problem

> [Sleeping Beauty](https://en.wikipedia.org/wiki/Sleeping_Beauty "Sleeping Beauty") volunteers to undergo the following experiment and is told all of the following details: On Sunday she will be put to sleep. Once or twice, during the experiment, Beauty will be awakened, interviewed, and put back to sleep with an amnesia-inducing drug that makes her forget that awakening. A [fair coin](https://en.wikipedia.org/wiki/Fair_coin "Fair coin") will be [tossed](https://en.wikipedia.org/wiki/Coin_flipping "Coin flipping") to determine which experimental procedure to undertake: if the coin comes up heads, Beauty will be awakened and interviewed on Monday only. If the coin comes up tails, she will be awakened and interviewed on Monday and Tuesday. In either case, she will be awakened on Wednesday without interview and the experiment ends.

> Any time Sleeping Beauty is awakened and interviewed she will not be able tell which day it is or whether she has been awakened before. During the interview Beauty is asked: "What is your [credence](https://en.wikipedia.org/wiki/Credence_%28statistics%29 "Credence (statistics)") now for the proposition that the coin landed heads?".

Rather than conceiving of a fixed history in which what has happened in the past must have happened, anthropics gives us a tool to assess the associated counterfactual situations. If you’re feeling ambitious, read Nick Bostrom’s Doctoral Dissertation, [Anthropic Bias](http://www.anthropic-principle.com/%3Fq%3Dbook/table_of_contents), for a much more in-depth exploration of the topic.

## The Coma Beauty Problem

Let us suppose that Sleeping Beauty fell into a coma in the mid 1980's. It is now 30 years later, and some neurochemists have discovered a means to awakening her. They administer their new drug, and she wakes.

Hoping to [ease her integration back into the world](https://www.youtube.com/watch?v=plgniAt36YY), the neurochemists adhere to the same testing protocol as the original experimenters and interview her as though she had been put to sleep earlier in the week. However, instead of asking about her credence about a coin flip, they explain that she was in a coma, and ask this, "What is your_[credence](https://en.wikipedia.org/wiki/Credence_%2528statistics%2529 "Credence (statistics)") now for the proposition that the Cold War resulted in a cataclysmic nuclear war?"

We observe that the Cold War between the United States and the Soviet Union did not culminate in a nuclear war. The anthropic logic here is that an observer is vastly more likely to live in a world where catastrophic nuclear war has not occurred, as such a war would vastly reduce the number of observers capable of making the observation. Stated differently, it’s way more likely that an observer observes a universe in which a global thermonuclear war was never fought than that an observer observes a universe in which it has.

Note that this says nothing about the _pre hoc_ probability of that war, other than that it was less than 1\. So, what _does_ this tell us? More or less nothing, I’m afraid. As [Roko Jelavic points out](https://www.facebook.com/rjelavic/posts/10211689902182034?match=cm9rbyBqZWxhdmnEhyxhbnRocm9waWMsamVsYXZpxIcscm9rbw%3D%3D), we shouldn't use our empirical observations about existential risks as a basis for reasoning about them:

> Let's take nukes as an example. Taking into account the number of close calls happening over the years* it's really strange nothing bad ever happened. Taking many-worlds interpretation of quantum mechanics into account, it's possible something did happen, but in some other Everett branch - and in that world, we went extinct. By anthropic principle, it's clear we find ourselves in a branch that did in fact not go extinct. When trying to estimate the existential risk of nukes, some people may take comfort in the fact no disaster happened until now, but if many-worlds is correct, you can't take history into account in that way. This generalizes to xrisk beyond just nukes.

# Conclusion

# References
