<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<title>Superintelligence Bibliography</title>
<!-- Material Design fonts -->
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/icon?family=Material+Icons">
<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.6/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.6/js/bootstrap.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/magnific-popup-1.1.0/magnific-popup.css" rel="stylesheet" />
<script src="site_libs/magnific-popup-1.1.0/jquery.magnific-popup.min.js"></script>
<link href="site_libs/bootstrap_material-0.1/bootstrap-material-design.min.css" rel="stylesheet" />
<link href="site_libs/bootstrap_material-0.1/ripples.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap_material-0.1/material.min.js"></script>
<script src="site_libs/bootstrap_material-0.1/ripples.min.js"></script>
<link href="site_libs/material-0.1/material.css" rel="stylesheet" />
<script src="site_libs/material-0.1/material.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; background-color: #dddddd; }
td.sourceCode { padding-left: 5px; }
code > span.kw { font-weight: bold; } /* Keyword */
code > span.dt { color: #800000; } /* DataType */
code > span.dv { color: #0000ff; } /* DecVal */
code > span.bn { color: #0000ff; } /* BaseN */
code > span.fl { color: #800080; } /* Float */
code > span.ch { color: #ff00ff; } /* Char */
code > span.st { color: #dd0000; } /* String */
code > span.co { color: #808080; font-style: italic; } /* Comment */
code > span.al { color: #00ff00; font-weight: bold; } /* Alert */
code > span.fu { color: #000080; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #ff0000; font-weight: bold; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #ff00ff; } /* SpecialChar */
code > span.vs { color: #dd0000; } /* VerbatimString */
code > span.ss { color: #dd0000; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { } /* Variable */
code > span.cf { } /* ControlFlow */
code > span.op { } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { font-weight: bold; } /* Preprocessor */
code > span.at { } /* Attribute */
code > span.do { color: #808080; font-style: italic; } /* Documentation */
code > span.an { color: #808080; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #808080; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #808080; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="site_libs/style.css" type="text/css" />

<!-- tabsets -->
<script src="/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<script src="/codefolding.js"></script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

</head>

<body>

<div class="header-panel shadow z-2">
    <div class="container-fluid">
        <div class="row">
            <div class="col-xs-3">
        <div id="header">
    <h1 class="title">Superintelligence Bibliography</h1>
                </div>
    </div>
</div>
</div>
</div>


<div class="container-fluid main-container">
    <div class="row">
      <nav class="col-xs-3 menu" id="toc">
        <ul>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#why-should-we-study-ai">Why Should We Study AI?</a></li>
        <li><a href="#what-epistemological-tools-will-we-need">What epistemological tools will we need?</a></li>
        <li><a href="#how-do-we-instill-values">How do we instill values?</a></li>
        <li><a href="#how-do-we-implement-it">How do we Implement it?</a></li>
        <li><a href="#how-do-we-move-forward">How do we move Forward?</a></li>
        <li><a href="#references-i-would-skip">References I would Skip</a></li>
        </ul>
      </nav>
     <div class="pages col-xs-9">
     <div class="row">
       <div class="col-xs-10">


<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Anthony A. Boyles</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">Essays</a>
</li>
<li>
  <a href="../code.html">Code</a>
</li>
<li>
  <a href="../about.html">About</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="../README.html">Meta</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">source</span>(<span class="st">&quot;infobox.R&quot;</span>)
meta &lt;-<span class="st"> </span><span class="kw">list</span>(
  <span class="st">&quot;Status&quot;</span> =<span class="st"> &quot;Woefully Incomplete&quot;</span>,
  <span class="st">&quot;Epistemic Status&quot;</span> =<span class="st"> &quot;Relatively Certain&quot;</span>,
  <span class="st">&quot;Last Revision&quot;</span> =<span class="st"> </span><span class="kw">file.info</span>(<span class="st">&quot;SuperintelligenceBibliography.Rmd&quot;</span>)$mtime
)
<span class="kw">infobox</span>(meta)</code></pre></div>
<div class="panel panel-default infobox">
<table class="panel-body table infoboxtable">
<tbody>
<tr>
<td>
<b>Status</b>
</td>
<td>Woefully Incomplete</td>
</tr>
<tr>
<td>
<b>Epistemic Status</b>
</td>
<td>Relatively Certain</td>
</tr>
<tr>
<td>
<b>Last Revision</b>
</td>
<td>2016-12-09 16:35:38</td>
</tr>
</tbody>
<tfoot></tfoot>
</table>
</div>
<p>This follows my self-education in Machine Intelligence, specifically as I would have ordered these materials if I were to start from the beginning again. This is roughly ordered by technical difficulty: the less technical books are at the top, and more technical works follow at the bottom. At the end, I’ve also included a section of books I simply wouldn’t bother with at all. This is going to have significant overlap with <a href="http://intelligence.org/research-guide/">MIRI’s Research Guide</a>, albeit with more non technical discussions about Machine Intelligences and General Intelligence. I intend to update it as often as I consume literature which is either useful for the study of Machine Superintelligence, or seem like it should be but isn’t (by virtue of being bad, misleading, or adding no value beyond other works already listed here).</p>
</div>
<div id="why-should-we-study-ai" class="section level1">
<h1>Why Should We Study AI?</h1>
<p>Machine Superintelligence falls into a class of dangerous things called Existential Risks. Basically, and existential risk is a thing that could wipe out humanity if it happened. Unfriendly AI is an Existential Risk. But unlike other Existential Risks, Machine Superintelligence also possesses the possibility of causing unbounded favorable changes for humanity, including working out protections against other Existential Risks. The best broad introduction to the problems of Machine Superintelligence comes from a little science blog called <a href="http://waitbutwhy.com/">Wait, but Why?</a></p>
<ul>
<li><em>Wait, but Why? <a href="http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html">The AI Revolution: The Road to Superintelligence</a>,</em> <em><a href="http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html">Our Immortality or Extinction</a></em></li>
<li><p>Muelhauser, Luke. <a href="http://lukemuehlhauser.com/a-reply-to-wait-but-why-on-machine-superintelligence/">A reply to <em>Wait But Why</em> on machine superintelligence</a></p></li>
<li><p>Armstrong, Stuart. <span style="text-decoration-line: underline;"><a href="http://www.amazon.com/dp/1939311098/ref=as_li_ss_tl?tag=miri05-20">Smarter than Us: The Rise of Machine Intelligence</a></span></p></li>
<li>Bostrom, Nick. <a href="http://www.ted.com/talks/nick_bostrom_what_happens_when_our_computers_get_smarter_than_we_are">What happens when our computers get smarter than we are?</a> (2015 TED Talk)</li>
<li>Bostrom, Nick. <a href="https://www.youtube.com/watch?v=pywF6ZzsghI">Superintelligence - Authors@Google</a></li>
<li>Boyles, Tony. <a href="https://storify.com/AABoyles/how-to-read-bostrom-s-superintelligence">How to Read Superintelligence</a></li>
<li>Bostrom, Nick. <a href="http://smile.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom-ebook/dp/B00LOOCGB2/ref=tmm_kin_title_0?_encoding=UTF8&amp;sr=&amp;qid="><span style="text-decoration-line: underline;">Superintelligence</span></a></li>
<li><p>Bostrom, Nick. <em><a href="http://www.nickbostrom.com/superintelligentwill.pdf">The Superintelligent Will: Motivated Instrumental Rational in Advanced Artificial Agents</a></em> (2012)</p></li>
</ul>
</div>
<div id="what-epistemological-tools-will-we-need" class="section level1">
<h1>What epistemological tools will we need?</h1>
<div id="rationality" class="section level2">
<h2>Rationality</h2>
<p>This first one may seem a bit surprising. Our brains are full of amusing little traps–Inconsistencies in our decision-making algorithms. These are called “biases.” Rationality is the ability to identify, circumvent, and/or ultimately counteract these biases. We don’t want biases to impede our ability to recognize potentially catastrophic pitfalls on the road to a Superintelligence.</p>
<ul>
<li>Gloor, Lucas. <a href="http://reg-charity.org/rationality-i/">Rationality: the Science of Winning, Part I</a>, <a href="http://reg-charity.org/rationality-ii/">Part II</a>, <a href="http://reg-charity.org/rationality-iii/">Part III</a></li>
<li>Yudkowsky, Eliezer. <a href="http://hpmor.com/">Harry Potter and the Methods of Rationality</a> - In an effort to spread rationality, Eliezer Yudkowsky wrote a Harry Potter Fan Fiction that gives the reader a powerful first-person perspective of a Rationalist boy-genius occupying the same time and space as the original Harry Potter. It’s very long but quite fun to read.</li>
<li>Soares, Nates. <a href="http://lesswrong.com/lw/m07/desire_is_the_direction_rationality_is_the/">Desire is the Direction, Rationality is the Magnitude</a>, <a href="http://lesswrong.com/lw/m0p/blind_artifacts/">Blind Artifacts</a>, <a href="http://lesswrong.com/lw/m0x/ephemeral_correspondence/">Ephemeral Correspondence</a>, <a href="http://lesswrong.com/lw/m19/the_path_of_the_rationalist/">The Path of the Rationalist</a> - Nate Soares wrote a series of four essays designed to provide aspiring rationalists with the background to understand Rationality as its methods are introduced.</li>
<li>Yudkowsky, Eliezer. <a href="https://intelligence.org/rationality-ai-zombies/">Rationality: From AI to Zombies</a> - Eliezer Yudkowsky spent the years between 2006 and 2009 writing near-daily blog posts as tutorials for identifying, understanding, and ultimately transcending bias, first at the group economics blog <a href="http://www.overcomingbias.com/">Overcoming Bias</a> and then at the group rationality blog <a href="http://lesswrong.com/top/?t=all">LessWrong</a>. These posts were organized into a set of so-called “Sequences,” which have been the foundational content of the LessWrong community. These sequences were subsequently edited and assembled into a more cohesive book.</li>
</ul>
</div>
<div id="game-theory" class="section level2">
<h2>Game Theory</h2>
<p>Telling an AI how to behave might work, but it’s not a plan upon which we should count. We’ll be much better off with an Intelligence that is imbued with a provably favorable approach to decision making. This includes its calculus about our thinking processes. This kind of thinking is typical of non-cooperative Game Theory.</p>
<ul>
<li>Davis, Morton D. <span style="text-decoration-line: underline;"><a href="http://smile.amazon.com/Game-Theory-Nontechnical-Introduction-Mathematics-ebook/dp/B008TVLME0/ref=sr_1_1?s=digital-text&amp;ie=UTF8&amp;qid=1422630584&amp;sr=1-1&amp;keywords=game+theory+a+nontechnical+introduction&amp;pebp=1422630590363&amp;peasin=B008TVLME0">Game Theory: A Nontechnical Introduction</a></span>.</li>
<li>Osborne, Martin and Ariel Rubinstein. <span style="text-decoration-line: underline;"><a href="http://www.economics.utoronto.ca/osborne/cgt/index.html">A Course in Game Theory</a></span>.</li>
<li>Axelrod, Robert. <span style="text-decoration-line: underline;"><a href="http://smile.amazon.com/Evolution-Cooperation-Revised-Robert-Axelrod-ebook/dp/B00AHFJ5VS/ref=tmm_kin_title_0?_encoding=UTF8&amp;sr=1-1&amp;qid=1422630418">The Evolution of Cooperation</a></span>. (2006)</li>
</ul>
</div>
<div id="decision-theory" class="section level2">
<h2>Decision Theory</h2>
<p>Game Theory tends to fixate on the calculation of equilibrium given rational actors (for whom “rational” <a href="http://en.wikipedia.org/wiki/Rational_choice_theory#Actions.2C_assumptions.2C_and_individual_preferences">simply means</a> “exhibits a consistent preference ordering without circular transitivities”). By contrast, Decision Theory employs much of the same mathematical formulations to engineer more favorable outcomes under circumstances in which traditional economic logic makes less sense. One of the most Fundamental problems in Decision Theory is <a href="http://en.wikipedia.org/wiki/Newcomb%27s_paradox">Newcomb’s Problem</a>.</p>
<ul>
<li>Yudkowsky, Eliezer. <a href="https://intelligence.org/files/TDT.pdf">Timeless Decision Theory</a></li>
<li>Yudkowsky et al. <a href="http://arxiv.org/abs/1401.5577">Robust Cooperation in the Prisoner’s Dilemma: Program Equilibrium via Provability Logic</a>. Arxiv</li>
</ul>
</div>
</div>
<div id="how-do-we-instill-values" class="section level1">
<h1>How do we instill values?</h1>
<p>(I’ll be expanding significantly on this section in the near future.)</p>
<ul>
<li>Yudkowsky, Eliezer. <a href="https://intelligence.org/files/CEV.pdf">Coherent Extrapolated Volition</a></li>
</ul>
</div>
<div id="how-do-we-implement-it" class="section level1">
<h1>How do we Implement it?</h1>
<p>Artificial Intelligence is a fairly specific subfield of Computer Science. The Market Share for AI applications has exploded in the past couple of years (in particular, <a href="http://www.iro.umontreal.ca/~bengioy/dlbook/">Deep Learning</a>). While these applications are still very far removed from the software required to build a Machine Superintelligence, they form the strongest foundation for ultimately building one.</p>
<ul>
<li>Russell, Stewart and Peter Norvig. <a href="http://smile.amazon.com/Artificial-Intelligence-Modern-Approach-ebook/dp/B008VIWTIY/ref=tmm_kin_title_0?ie=UTF8&amp;qid=1353970431&amp;sr=8-1&amp;sa-no-redirect=1">AI: A Modern Approach</a></li>
</ul>
<div id="aixi" class="section level2">
<h2>AIXI</h2>
<ul>
<li>Hutter, Marcus. <em><a href="http://arxiv.org/abs/cs/0004001">A Theory of Universal Artificial Intelligence based on Algorithmic Complexity</a></em>. (2001) - Building a General AI is a difficult problem. Marcus Hutter lowered the bar a bit by devising an algorithmic formalization of <a href="https://en.wikipedia.org/wiki/Occam%27s_razor">Occam’s Razor</a>.</li>
</ul>
</div>
<div id="other" class="section level2">
<h2>Other</h2>
<ul>
<li>Wissner-Gross, A.D. <a href="http://www.ted.com/talks/alex_wissner_gross_a_new_equation_for_intelligence">A New Equation for Intelligence</a>. (2014 TED Talk)</li>
<li>Wissner-Gross, A.D. and C.E. Freer. <em><a href="http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.110.168702">Causal Entropic Forces</a></em>. Phys. Rev. Lett. 110 <a href="http://www.alexwg.org/link?url=http%3A%2F%2Fwww.alexwg.org%2Fpublications%2FPhysRevLett_110-168702.pdf">[ungated]</a></li>
</ul>
</div>
</div>
<div id="how-do-we-move-forward" class="section level1">
<h1>How do we move Forward?</h1>
<ul>
<li><a href="http://intelligence.org/all-publications/">MIRI’s Research Corpus</a></li>
<li><a href="http://agentfoundations.org/">The Intelligent Agent Foundation Forum</a></li>
</ul>
</div>
<div id="references-i-would-skip" class="section level1">
<h1>References I would Skip</h1>
<p>There are a lot of people writing about AI, and they have lots of good ideas. There are some bad ideas, but mostly there are useless ideas. If something I have read doesn’t contribute to the conversation (but seems popular enough not to ignore), it goes here. Sorry, Ray Kurzweil.</p>
<ul>
<li>Kurzweil, Ray. <span style="text-decoration-line: underline;"><a href="http://smile.amazon.com/Age-Spiritual-Machines-Computers-Intelligence-ebook/dp/B002CIY8JW/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;sr=&amp;qid=">The Age of Spiritual Machines</a></span></li>
<li>Barrat, James. <a href="http://smile.amazon.com/Our-Final-Invention-Artificial-Intelligence-ebook/dp/B00CQYAWRY/ref=pd_sim_351_5?ie=UTF8&amp;refRID=0DJBFWW8B70JXPV2XTMX">Our Final Invention</a> - Barrat does an excellent job of introducing the problems of Superintelligence (although he occasionally warps the lexicon of the field in some weird ways). Unfortunately, as he trudges further into the book, the themes, points, and perspectives become increasingly scattered. This permits an interesting system of dialogues between experts from a series of interviews he did (with many of the names mentioned on this page), but leaves us without a cohesive picture of the control problem and the route to solutions. In lieu of this, I’d recommend reading Armstrong’s <span style="text-decoration: underline;">Smarter Than Us</span> (see above).</li>
<li>Chace, Calum. <a href="http://smile.amazon.com/Surviving-AI-promise-artificial-intelligence/dp/0993211623/ref=sr_1_1?ie=UTF8&amp;qid=1449343792&amp;sr=8-1&amp;keywords=Surviving+AI">Surviving AI</a> - This is an non-fictional exposition to accompany Chace’s novel, <a href="http://smile.amazon.com/Pandoras-Brain-Calum-Chace-ebook/dp/B00U4S4W7A/ref=sr_1_2?ie=UTF8&amp;qid=1449343792&amp;sr=8-2&amp;keywords=Surviving+AI">Pandora’s Brain</a>. While I haven’t read the fictional counterpart, Surviving AI is a perfectly adequate introduction to the AI control problem and its relationship to the broader AI community. Unfortunately, it doesn’t accomplish this even as well as Barrat’s <span style="text-decoration: underline;">Our Final Invention</span> (see above).</li>
</ul>
</div>

    </div>
    <div class="col-xs-2">
        <div class="btn-toolbar">
    <div class="btn-group">
    <a href="#" class="btn btn-raised btn-sm dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span class="text-muted">Code</span> <span class="text-muted caret"></span></a>
    <ul class="dropdown-menu" style="min-width: 50px;">
    <li><a id="rmd-show-all-code" href="#">Show All</a></li>
    <li><a id="rmd-hide-all-code" href="#">Hide All</a></li>
    </ul>
    </div>
    </div>
        </div>
  </div>
  </div>
  </div>
  <div class="row">
    </div>
  </div>

<script>
$(document).ready(function () {
  // add bootstrap table styles to pandoc tables
  $('tr.header').parent('thead').parent('table').addClass('table table-striped table-hover');

    var images = $('.pages img');
  images.filter(function() {
      if ($(this).parent().attr("class") == "figure") {
          return(false)
      } else {
          return(true);
      }
  }).wrap("<div class='figure'></div>");
  images.addClass("image-thumb").wrap("<div class='panel-body'></div>");
  $('.figure p.caption').wrap("<div class='panel-footer'></div>");
  $('.figure').addClass('panel panel-default');
  
    $('.pages img')
 	  .addClass("image-lb");
  $('.pages').magnificPopup({
	      type:'image',
	      closeOnContentClick: false,
	      delegate: 'img',
	      gallery: {enabled: false },
          removalDelay: 500,
          callbacks: {
              beforeOpen: function() {
                // just a hack that adds mfp-anim class to markup
                this.st.image.markup = this.st.image.markup.replace('mfp-figure', 'mfp-figure mfp-with-anim');
              }
          },
          mainClass: 'mfp-move-from-top',
	      image: {
	        verticalFit: true,
            titleSrc: 'alt'
	      }
 	    });
 	
    window.page = window.location.hash;
    if (window.page != "") {
      $(".menu").find("li[data-target=" + window.page + "]").trigger("click");
    }

    /* init material bootstrap js */
    $.material.init();
});
</script>




<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
