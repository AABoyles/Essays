---
title: "The Honest Truth about the Trolley Problem"
---

# Introduction

I think it’s time we faced a hard truth about the [Trolley Problem](https://en.wikipedia.org/wiki/Trolley_problem). Consider:

1.  If you asked me what I would do in the Trolley Problem, I would tell you that I would pull the lever.
2.  If I were _actually_ in the trolley problem, I would pull the lever.

However:

1.  If you asked me what I would do in the [fat man variant of the Trolley Problem](https://en.wikipedia.org/wiki/Trolley_problem%23The_fat_man), I would tell you that I would push the fat man.
2.  If I were _actually_ in the fat man variant of the Trolley Problem, I would _not_ push the fat man.

In other words, I’m _definitely_ a utilitarian, but I’m kinda, sorta lying about being a utilitarian.

# Thought Experiments are Terrible.

There aren’t many people who rebel against the classic formulation of the Trolley problem. But people seem to hate the fat man variant, and fight against the circumstances of the problem as soon as you point out the moral inconsistency to which it leads. The problem with the fat man variant is that it strains credulity. If a trolley were actually barreling towards five unsuspecting, utterly clueless people, impeding said trolley by dropping a fat man in front of it isn’t going to save our quintet of hopelessly inattentive fools. Sadly, I think, most of the thought experiments in moral philosophy exhibit this issue.

While I don’t have time to discuss every thought experiment in the field, there are a lot of examples. Just to pick on another one, let’s look at a variant of the [drowning child problem](http://www.utilitarian.net/singer/by/199704--.htm). Say, for example, that I wish to maximize human life-years. I’m wearing a $10,000 suit which I mean to sell, and use the money to [buy long-lasting insecticide nets](https://www.againstmalaria.com/), saving [approximately three lives](http://www.givewell.org/international/top-charities/amf%23Costperlifesaved) from malaria. However, en route to the place in which I mean to sell the suit, I pass a stream in which I see a small child drowning. I could save the child, but I know that if I get the suit wet, it will get muddy and shrink and lose almost all of its value. Because the child will live at most one life, whereas the people I save from malaria will live three, the utilitarian course of action is to avoid the stream, sell the suit, and donate the money.

But this runs strongly against our moral intuitions, and so we can rationalize our way to saving the drowning child. Nobody routinely wears $10,000 suits, so the situation is unrealistic. The resale value of the suit would be a fraction of that, so you’d save fewer than three lives (very pessimistically, less than one life, which justifies saving the drowning child). Water damage to a suit, even a very fine suit, is likely to be largely reversible. Etc.

What’s actually going on in your head is _not_ these rationalizations. It sounds more like this: "Oh no, there’s a child in front of me, _drowning!_ I had better save her!" When the utilitarian onlooker points out that more lives could be saved if you do not save the child, your brain rejects that reasoning on the basis that not saving drowning children is [disgusting](https://en.wikipedia.org/wiki/Disgust%23Morality). All the other logic follows, but it’s the drowning child’s _availability_ (related: [availability heuristic](https://en.wikipedia.org/wiki/Availability_heuristic)) coupled with that sense of disgust that actually makes the decision for you.

The error in the decision process is that we decide and then justify, rather than weigh the options and then decide. This leaves us vulnerable to moral [dutch book scams](https://en.wikipedia.org/wiki/Money_pump). I don’t know that I’ve heard this particular issue discussed in literature, but if I were inventing the term, I’d call it something like “presence bias”. You see, the drowning child is _present_. Her life or death is a decision _you_ must consciously and immediately make, and then live with the consequences. The people who die of malaria because of your decision to save the drowning child aren’t present, even though their lives and deaths are just as real and just as morally significant.

# No, really. Terrible.

In order to dissolve some of these, we ask people to accept the circumstances of the thought experiment at face value. We ask people to trust that there’s a reason the experiment is structured in this way, and disallows most criticism of this variety. In short, we ask people to consider themselves to occupy [The Least Convenient Possible World](http://lesswrong.com/lw/2k/the_least_convenient_possible_world/).

As a utilitarian, here’s the annoying thing: _those rationalizations are correct_. Rationalizations often are! We have spent much time devising a canon of thought experiments cleverly designed to force people into morally inconsistent decisions, but I don’t think we’ve spent enough time dealing with the fact that these thought experiments are really good at illustrating moral conundrums, and really, _really_ bad at representing the kinds of morally-important decisions people make on a daily basis.

To me, the most obvious example of this phenomenon is [the thought experiment of the unwilling organ donor](http://philosophyfaculty.ucsd.edu/faculty/rarneson/Courses/thomsonTROLLEY.pdf):

> Imagine yourself to be a surgeon, a truly great surgeon. Among other things you do, you transplant organs, and you are such a great surgeon that the organs you transplant always take. At the moment you have five patients who need organs. Two need one lung each, two need a kidney each, and the fifth needs a heart. If they do not get those organs today, they will all die; if you find organs for them today, you can transplant the organs and they will all live. But where to find the lungs, the kidneys, and the heart? The time is almost up when a report is brought to you that a young man who has just come into your clinic for his yearly check-up has exactly the right blood-type, and is in excellent health. Lo, you have a possible donor. All you need do is cut him up and distribute his parts among the five who need them. You ask, but he says, “Sorry. I deeply sympathize, but no.” Would it be morally permissible for you to operate anyway?

[Ed. Note: There’s an important bug in this formulation of the experiment. If the five patients are all matches for the young man, _the five patients are all matches for each other_. The correct escape hatch is to dissect one of the patients for her organs, saving four people and leaving the healthy young fellow alone. For the purposes of this discussion, let’s assume the story has been modified in some way to prohibit this possibility.]

The "gut" answer is that it would _not_ be moral to harvest the young man’s organs. In the real world, organs do need to be precise (and thereby relatively rare) matches for their new hosts. And even then, they will often be rejected. In the real world, the young man can live one quality life, or the patients can live five much less long, much more miserable lives. In the real world, harvesting a healthy, living person’s organs is illegal, and thereby comes with morally-significant consequences (like a reduced ability to practice medicine, imprisonment, or death).

We could do some math to figure out which scenario resulted in more life-years lived, but the point remains: In our sanitized, inconvenient world where surgery is a perfected practice, organs are never rejected, and life-years need not be quality adjusted, it is morally correct to harvest this man’s organs. In fact, it isn’t just correct; it’s obligatory.

These thought experiments are terrible because they’re designed to push you into a decision that have a clear, morally-optimal outcome if you accept the circumstances of the situation, but the circumstances run against the grain of our intuitions about these types of situations. They’ve been [carefully crafted to subvert protestations](http://www.faculty.virginia.edu/haidtlab/articles/manuscripts/haidt.bjorklund.working-paper.when%2520intuition%2520finds%2520no%2520reason.pub603.doc) based on exogenous information. Their terrible-ness is a feature, not a bug. If it weren’t, people would always select the morally-optimal course of action and all the moral philosophers would be out of jobs.

So, if our thought experiments are terrible, why don’t we try to come up with some more real-world examples? OK, here’s one: You can donate money to [the charity that is most efficient at saving people’s lives](http://www.givewell.org/charities/top-charities), or you can do [literally anything else with your money](https://www.amazon.com/).

# And I'm a Hypocrite.

Many philosophers have pointed out that utilitarianism is [philosophically demanding](https://en.wikipedia.org/wiki/Utilitarianism%23Too_demanding). Unlike [deontological moral codes](http://plato.stanford.edu/entries/ethics-deontological/), which [allow freedom within the bounds of their rules](http://plato.stanford.edu/entries/ethics-deontological/%23AdvDeoThe), utilitarianism demands that people optimize something based on some set of values to the exclusion of other values against which moral calculations are not made.

I attended [a lecture by Peter Singer](https://www.youtube.com/watch%3Fv%3DqsAOlPLlw1M) in which he fielded a question to the effect of, “You periodically fly between the US and Australia to visit your grandchildren. Why not donate the money you spend on flights to effective causes?” Singer first justified his routine flight schedule with a mention of his lecture circuit, which does indeed have a rather high expected value for effective charities. However, he also voluntarily conceded that his Australia trips didn’t necessarily serve this purpose, but that he loves his grandkids and enjoys spending time with them. The patriarch of [practiced, ethical living](https://smile.amazon.com/Writings-Ethical-Life-Peter-Singer/dp/0060007443/ref%3Dsr_1_1%3Fie%3DUTF8%26qid%3D1471881176%26sr%3D8-1%26keywords%3DWritings%2Bon%2Ban%2BEthical%2BLife) to the extreme still exhibits the same moral foibles as all the rest of us.

Which brings us back to me. I live a lavish lifestyle. It isn’t spectacularly lavish by current Western standards: I live in a comfortable 3 bedroom, 1.5 bathroom single family home near Washington D.C., on which I pay a mortgage which is approximately equal to the price of renting a similar home in the same area. But there are cheaper housing options. I could rent a one-bedroom or studio comparatively close by for half as much as I pay in mortgage payments. By donating that money, I could _save multiple lives_. Every year.

I have no answer to this. If we think of this as a sort of Trolley Problem (And it’s a particularly bad fit as moral conundrums go, but just work with me), I say I’ll pull the lever, but then I don’t. And I feel _horrible_ about it. Seriously.

But even if I didn’t have an intrinsic moral sense that this sort of behavior was wrong, I’d worry about adopting this type of decision process. I’m acutely aware that someday, I’ll be the person on the tracks, instead of the person controlling the lever.

![The selfish reason for utilitarianism](https://cdn-images-1.medium.com/max/800/1*BEYuF9HremGUMy24Bgi7Wg.png)
