---
title: "Common Failure Modes for ASI Safety Schemes"
---

# Abstract

Nearly all proposed solutions and ideas for sub-solutions to safely interacting with machine superintelligences lead to failure, with potentially catastrophic consequences. The modes by which these ideas fail can be grouped into a small number of classes. This paper offers a rubric for criticizing solutions to superintelligence-related problems by defining these classes.

# Introduction

Stuart Armstrong laid out eight common bases for rejecting as idea for AI control \cite{Armstrong2016-aj}:

* It's not well-defined.
* The setup can be hacked.
* Humans can be manipulated, hacked, or seduced.
* The design is not stable.
* The agent has, or will develop, dangerous goals.
* The agent will resist changes.
* The AI is much smarter than us.

# Definition

Solutions to the control problem or any of its sub-problems must ultimately be specified in code if they are to work. For example, consider the case of a human who issues the command "Do what I mean" to a machine superintelligence. In order to comply, the superintelligence must posses a sufficiently comprehensive model of the person to predict with high accuracy what the human meant.

# Hacking

## By the agent

Steve Omohundro has identified hacking as one an AI's primary drives or universally convergent goals [@Omohundro2008-ie]. Almost no utility function will offer a net reduction in utility to an agent which discovers how to manipulate systems in ways which were not intended by its creators.

## By outsiders, including other AI

Aum Shinrikio

Other AIs 

Former versions of itself

## Adding restrictions encourages the AI to hack them


# Humans

Many putative solutions revolve around the concept of keeping a "human in the loop". 

Eliezer Yudkowsky boiled this down to the smallest possible unit, proposing a boxed superintelligence restricted to a terminal accessible to a single human. The human operator, in turn, could transmit only a single bit of data to the outside world.

Irrationality. Humans possess a number of cognitive biases which inhibit us from acting as perfectly rational agents. These are exploitable from the perspective of a superintelligent

Untrustworthiness. Nick Bostrom proposed a control system in which a machine superintelligence would perform actions in exchange of "cryptographic reward tokens", some sort of special data which the superintelligent agent can't compute as easily as it can perform the actions requested by the human.

The AI Box experiment. Eliezer Yudkowsky explored this topic with a social experiment. He proposed to 

Basically, Humans suck.

# Stability

## Under self-modification

This limits the possibility for bootstrapping a superintelligence using stochastic, evolutionary methods.

This is also a well-documented obstacle to the creation of superintelligence. (Lob's theorem)

## Under sub-agent creation

This is also a well-documented obstacle to the creation of superintelligence.

# Goals

Most goals that people generate are akin to "Maximize human happiness", "Minimize human suffering", "Maximize human life". These fail according to the first problem, definition. How should the agent define happiness, suffering, or life? We don't have rigorous definitions of these concepts which 

# Alteration

Given a goal, an advanced agent will pursue that goal to the exclusion of every other possible goal.

The Paperclip Maximizer. 

# Superintelligence

In order to meet the prevailing definition "superintelligence", an agent must be smarter than us.

Consider the case of a prisoner with exceptionally high intelligence, being guarded by people of merely average intelligence.

Let us suppose that besides being highly intelligent, the prisoner possessed resources 

# Conclusion

# References
