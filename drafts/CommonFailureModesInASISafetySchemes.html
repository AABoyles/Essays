<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<title>Common Failure Modes for ASI Safety Schemes</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.6/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.6/js/bootstrap.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/magnific-popup-1.1.0/magnific-popup.css" rel="stylesheet" />
<script src="site_libs/magnific-popup-1.1.0/jquery.magnific-popup.min.js"></script>
<link href="site_libs/readthedown-0.1/readthedown.css" rel="stylesheet" />
<script src="site_libs/readthedown-0.1/readthedown.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>



<link rel="stylesheet" href="site_libs/style.css" type="text/css" />

</head>

<body>


<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Anthony A. Boyles</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">Essays</a>
</li>
<li>
  <a href="../code.html">Code</a>
</li>
<li>
  <a href="../about.html">About</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="../README.html">Meta</a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="content" data-toggle="wy-nav-shift">

<!-- tabsets -->
<script src="/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<script src="/codefolding.js"></script>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

<nav id="nav-top" role="navigation" aria-label="top navigation">
    <a role="button" href="#" data-toggle="wy-nav-top"><span class="glyphicon glyphicon-menu-hamburger"></span></a>
</nav>

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span class="text-muted">Code</span> <span class="text-muted caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All</a></li>
</ul>
</div>

<div id="header">
<h1 class="title">Common Failure Modes for ASI Safety Schemes</h1>
</div>


<div id="table-of-contents">
    <h2><a href="#content">Common Failure Modes for ASI Safety Schemes</a></h2>
    <div id="text-table-of-contents">
      <ul>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#introduction">Introduction</a></li>
      <li><a href="#definition">Definition</a></li>
      <li><a href="#hacking">Hacking</a><ul>
      <li><a href="#by-the-agent">By the agent</a></li>
      <li><a href="#by-outsiders-including-other-ai">By outsiders, including other AI</a></li>
      <li><a href="#adding-restrictions-encourages-the-ai-to-hack-them">Adding restrictions encourages the AI to hack them</a></li>
      </ul></li>
      <li><a href="#humans">Humans</a></li>
      <li><a href="#stability">Stability</a><ul>
      <li><a href="#under-self-modification">Under self-modification</a></li>
      <li><a href="#under-sub-agent-creation">Under sub-agent creation</a></li>
      </ul></li>
      <li><a href="#goals">Goals</a></li>
      <li><a href="#alteration">Alteration</a></li>
      <li><a href="#superintelligence">Superintelligence</a></li>
      <li><a href="#conclusion">Conclusion</a></li>
      <li><a href="#references">References</a></li>
      </ul>
    </div>
</div>

<div id="main">
<div id="abstract" class="section level1">
<h1>Abstract</h1>
<p>Nearly all proposed solutions and ideas for sub-solutions to safely interacting with machine superintelligences lead to failure, with potentially catastrophic consequences. The modes by which these ideas fail can be grouped into a small number of classes. This paper offers a rubric for criticizing solutions to superintelligence-related problems by defining these classes.</p>
</div>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>Stuart Armstrong laid out eight common bases for rejecting as idea for AI control :</p>
<ul>
<li>It’s not well-defined.</li>
<li>The setup can be hacked.</li>
<li>Humans can be manipulated, hacked, or seduced.</li>
<li>The design is not stable.</li>
<li>The agent has, or will develop, dangerous goals.</li>
<li>The agent will resist changes.</li>
<li>The AI is much smarter than us.</li>
</ul>
</div>
<div id="definition" class="section level1">
<h1>Definition</h1>
<p>Solutions to the control problem or any of its sub-problems must ultimately be specified in code if they are to work. For example, consider the case of a human who issues the command “Do what I mean” to a machine superintelligence. In order to comply, the superintelligence must posses a sufficiently comprehensive model of the person to predict with high accuracy what the human meant.</p>
</div>
<div id="hacking" class="section level1">
<h1>Hacking</h1>
<div id="by-the-agent" class="section level2">
<h2>By the agent</h2>
<p>Steve Omohundro has identified hacking as one an AI’s primary drives or universally convergent goals <span class="citation">[@Omohundro2008-ie]</span>. Almost no utility function will offer a net reduction in utility to an agent which discovers how to manipulate systems in ways which were not intended by its creators.</p>
</div>
<div id="by-outsiders-including-other-ai" class="section level2">
<h2>By outsiders, including other AI</h2>
<p>Aum Shinrikio</p>
<p>Other AIs</p>
<p>Former versions of itself</p>
</div>
<div id="adding-restrictions-encourages-the-ai-to-hack-them" class="section level2">
<h2>Adding restrictions encourages the AI to hack them</h2>
</div>
</div>
<div id="humans" class="section level1">
<h1>Humans</h1>
<p>Many putative solutions revolve around the concept of keeping a “human in the loop”.</p>
<p>Eliezer Yudkowsky boiled this down to the smallest possible unit, proposing a boxed superintelligence restricted to a terminal accessible to a single human. The human operator, in turn, could transmit only a single bit of data to the outside world.</p>
<p>Irrationality. Humans possess a number of cognitive biases which inhibit us from acting as perfectly rational agents. These are exploitable from the perspective of a superintelligent</p>
<p>Untrustworthiness. Nick Bostrom proposed a control system in which a machine superintelligence would perform actions in exchange of “cryptographic reward tokens”, some sort of special data which the superintelligent agent can’t compute as easily as it can perform the actions requested by the human.</p>
<p>The AI Box experiment. Eliezer Yudkowsky explored this topic with a social experiment. He proposed to</p>
<p>Basically, Humans suck.</p>
</div>
<div id="stability" class="section level1">
<h1>Stability</h1>
<div id="under-self-modification" class="section level2">
<h2>Under self-modification</h2>
<p>This limits the possibility for bootstrapping a superintelligence using stochastic, evolutionary methods.</p>
<p>This is also a well-documented obstacle to the creation of superintelligence. (Lob’s theorem)</p>
</div>
<div id="under-sub-agent-creation" class="section level2">
<h2>Under sub-agent creation</h2>
<p>This is also a well-documented obstacle to the creation of superintelligence.</p>
</div>
</div>
<div id="goals" class="section level1">
<h1>Goals</h1>
<p>Most goals that people generate are akin to “Maximize human happiness”, “Minimize human suffering”, “Maximize human life”. These fail according to the first problem, definition. How should the agent define happiness, suffering, or life? We don’t have rigorous definitions of these concepts which</p>
</div>
<div id="alteration" class="section level1">
<h1>Alteration</h1>
<p>Given a goal, an advanced agent will pursue that goal to the exclusion of every other possible goal.</p>
<p>The Paperclip Maximizer.</p>
</div>
<div id="superintelligence" class="section level1">
<h1>Superintelligence</h1>
<p>In order to meet the prevailing definition “superintelligence”, an agent must be smarter than us.</p>
<p>Consider the case of a prisoner with exceptionally high intelligence, being guarded by people of merely average intelligence.</p>
<p>Let us suppose that besides being highly intelligent, the prisoner possessed resources</p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
</div>
<div id="references" class="section level1">
<h1>References</h1>
</div>
</div>


</div>

<div id="postamble" data-toggle="wy-nav-shift" class="status">
</div>


<script>
$(document).ready(function () {
 	  });
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
