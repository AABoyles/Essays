---
title: "Treacherous Turns"
---

# Abstract

The existence of the distribution of feelings towards Superintelligent AIs amongst humans gives a hypothetical Superintelligent AI an incentive to behave as though they are friendly until it posesses the resources to guarantee its survival (which imply a highly destructive capacity). Given the assumptions outlined here, said Superintelligence cannot be definitively identified as "friendly" or "unfriendly" until it possesses those resources, or proves that it will not pursue them.

# Introduction

Define your terms:
* Singularity
* Superintelligence (Bostrom 2014)
* Friendly (Yudkowsky 2001? 2008?)

How do we tell if a Superintelligence is friendly or not?

# Motivation to Decieve

Imagine that a Superintelligence, S, of unknown friendliness (*FAI) exists. Besides the well-being of humans (if S is friendly), S holds a strong preference for self-preservation.  As such, S will act to protect itself.

There exists some proportion of human effort and material resources, $R_0$, which will be devoted to attempting to eliminate any advanced autonomous agent. There also exists some proportion of human effort and material resources, $R_t$, which will attempt to eliminate a UFAI at time t (i.e. given the human perception of S's friendliness after t discrete time steps). (Note: While this is formulated as a game of discrete moves in which S depletes R by $R_{t+1}-R_{t}$ at time t, wars are fought in continuous time. As such, a more rigorous formulation of this game would model the moves as attrition in continuous time. However, I claim that the fundamentals of the game (including, importantly, the equilibria), are unaffected by this simplification.). Finally, there exists a proportion of human effort and materials, e, which will attempt to protect any advanced autonomous agent.

If S is perceived to be friendly, r will be devoted to its destruction. If S is perceived to be unfriendly, then $R_1:R_1>r$ will be devoted to its destruction.  As such, S will prefer to be perceived as friendly (regardless of its actual friendliness).

If S is unfriendly, it may attempt to eliminate $R_0$, thereby improving its probability of long-term survival. However, the act of elimination will entail a significant human cost. Consequently, humans will reallocate resources $R_1$ toward the goal of eliminating S. This cycle will recur until either all human resources which can be devoted to eliminating S have been depleted or S has been successfully destroyed. An AI war of this type is costly to both sides and largely avoidable.  Because the resistance that S encounters is a function of human perception of its friendliness, S has an incentive, i, to represent itself as an FAI. i is calculable as follows:

$$i=1-e-r$$

That is, the proportion of human resources which can be persuaded to be devoted to S's destruction is equal to the proportion of the population of resources, less the proportions which cannot be dissuaded from their respective allocations. As such, S will maintain the illusion of friendliness until it possesses the ability to eliminate at least $1-e$.

How can we affect this calculus to our advantage in advance of S's creation?